--- a/streamlit_app.py
+++ b/streamlit_app.py
@@
-import os
-import io
-import csv
+import os
+import io
+import csv
+import glob
+import numpy as np
@@
 import streamlit as st
 import google.generativeai as genai
+try:
+    import faiss  # optional: ì—†ìœ¼ë©´ NumPy í´ë°±
+except Exception:
+    faiss = None
+from sentence_transformers import SentenceTransformer
@@
 DEFAULT_MODEL = "gemini-2.5-flash"
@@
 def ensure_data_evidence(prompt: str) -> str:
@@
     return updated
+
+# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
+# ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤(subtitle_summary) ë¡œë” + ê²€ìƒ‰
+# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
+@st.cache_resource(show_spinner=False)
+def load_external_kb(dir_path: str, embed_model: str = None):
+    """subtitle_summary/summary í´ë”ì˜ JSONë“¤ì„ ì„ë² ë”©/ìƒ‰ì¸. ì—†ìœ¼ë©´ None."""
+    model_name = os.getenv("EMBED_MODEL_NAME", embed_model or "jhgan/ko-sbert-nli")
+    files = glob.glob(os.path.join(dir_path, "*.json"))
+    docs, metas = [], []
+    for fp in files:
+        try:
+            with open(fp, "r", encoding="utf-8") as f:
+                data = json.load(f)
+            if isinstance(data, dict):
+                data = [data]
+            for i, d in enumerate(data):
+                ctx = d.get("context", {})
+                prob = d.get("problem", [])
+                sol = d.get("solution", [])
+                text = f"ìƒí™©: {ctx}\në¬¸ì œì : {prob}\ní•´ê²°: {sol}"
+                docs.append(text)
+                metas.append((os.path.basename(fp), i))
+        except Exception:
+            continue
+    if not docs:
+        return None
+    model = SentenceTransformer(model_name)
+    doc_emb = model.encode(docs, normalize_embeddings=True)
+    doc_emb = np.asarray(doc_emb, dtype=np.float32)
+    if faiss is not None:
+        index = faiss.IndexFlatIP(doc_emb.shape[1])
+        index.add(doc_emb)
+    else:
+        index = None
+
+    def search(query: str, top_k: int = 3) -> str:
+        if not query:
+            return ""
+        q = model.encode([query], normalize_embeddings=True)
+        q = np.asarray(q, dtype=np.float32)
+        if index is not None:
+            D, I = index.search(q, top_k)
+            idxs = I[0]
+        else:
+            sims = doc_emb @ q[0]
+            idxs = np.argsort(-sims)[:top_k]
+        lines = ["[ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ ìƒìœ„ ê·¼ê±°]"]
+        for rank, idx in enumerate(idxs, 1):
+            if idx < 0 or idx >= len(docs):
+                continue
+            file, j = metas[idx]
+            snippet = docs[idx].replace("\n", " ")[:200]
+            lines.append(f"{rank}. íŒŒì¼:{file} #{j} | {snippet}")
+        return "\n".join(lines)
+
+    return {"search": search}
+
+def build_kb_query(info: dict, extra: str = "") -> str:
+    """ìƒì  ì¹´ë“œ + ì¶”ê°€ ë¬¸êµ¬ë¡œ KB ì§ˆì˜ë¬¸ ìƒì„±."""
+    keys = ["ìƒì ëª…", "ì—…ì¢…", "í”„ëœì°¨ì´ì¦ˆì—¬ë¶€", "ì í¬ì—°ë ¹", "ê³ ê°ì—°ë ¹ëŒ€", "ê³ ê°í–‰ë™"]
+    parts = [f"{k}:{info[k]}" for k in keys if info.get(k)]
+    if extra:
+        parts.append(str(extra))
+    return " ".join(parts)
@@
 def render_mct_tab():
@@
-    # ìƒì„± ë²„íŠ¼
+    # ìƒì„± ë²„íŠ¼
     generate = st.button("ğŸš€ ì „ë¬¸ ì†”ë£¨ì…˜ ìƒì„±", use_container_width=True, disabled=not encoded_mct)
     if generate:
@@
-        prompt = build_mct_consult_prompt(st.session_state.mct_info, encoded_mct, p_main, p_updn)
+        prompt = build_mct_consult_prompt(st.session_state.mct_info, encoded_mct, p_main, p_updn)
+        # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
+        kb_opts = st.session_state.get("_kb_opts", {})
+        if kb_opts.get("use"):
+            kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
+            if kb:
+                kb_q = build_kb_query(st.session_state.mct_info, encoded_mct)
+                kb_ev = kb["search"](kb_q, top_k=int(kb_opts.get("topk", 3)))
+                if kb_ev:
+                    prompt += f"\n\n{kb_ev}\n"
@@
-    # í›„ì† ì§ˆë¬¸
+    # í›„ì† ì§ˆë¬¸
     mct_q = st.chat_input("ENCODED_MCT ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", key="mct_chat_input")
     if mct_q:
@@
-        with st.chat_message("assistant"):
+        with st.chat_message("assistant"):
             ph2 = st.empty()
-            ans = stream_gemini(
+            # ì™¸ë¶€ KB ê·¼ê±° (ì˜µì…˜) ì¶”ê°€
+            kb_opts = st.session_state.get("_kb_opts", {})
+            if kb_opts.get("use"):
+                kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
+                if kb:
+                    base_follow += "\n\n" + kb["search"](build_kb_query(st.session_state.mct_info, mct_q),
+                                                          top_k=int(kb_opts.get("topk", 3)))
+            ans = stream_gemini(
                 base_follow,
@@
 st.set_page_config(page_title="AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸", layout="wide")
 st.title("ğŸ’¬ AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸")
 
-# ğŸ‘‰ ì‚¬ì´ë“œë°” ëª¨ë“œ ì„ íƒ: ê¸°ì¡´ ìƒë‹´ / ENCODED_MCT ì»¨ì„¤í„´íŠ¸
+# ğŸ‘‰ ì‚¬ì´ë“œë°” ëª¨ë“œ ì„ íƒ: ê¸°ì¡´ ìƒë‹´ / ENCODED_MCT ì»¨ì„¤í„´íŠ¸
 mode = st.sidebar.radio("ëª¨ë“œ", ["ê¸°ì¡´ ìƒë‹´", "ENCODED_MCT ì»¨ì„¤í„´íŠ¸"], index=0)
+# ğŸ” ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ ì˜µì…˜
+st.sidebar.divider()
+st.sidebar.markdown("**ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤(subtitle_summary) ì‚¬ìš©**")
+use_kb = st.sidebar.checkbox("ê·¼ê±° ì£¼ì… ì‚¬ìš©", value=False)
+kb_dir = st.sidebar.text_input("KB í´ë” ê²½ë¡œ", "./subtitle_summary/summary", disabled=not use_kb)
+kb_topk = st.sidebar.slider("ê·¼ê±° ê°œìˆ˜", 1, 5, 3, disabled=not use_kb)
+st.session_state["_kb_opts"] = {"use": use_kb, "dir": kb_dir, "topk": kb_topk}
 if mode == "ENCODED_MCT ì»¨ì„¤í„´íŠ¸":
     render_mct_tab()
     st.stop()
@@
-        if persona and "prompt" in persona:
-            prompt = ensure_data_evidence(persona["prompt"])
-        else:
-            prompt = ensure_data_evidence(
+        if persona and "prompt" in persona:
+            prompt = ensure_data_evidence(persona["prompt"])
+        else:
+            prompt = ensure_data_evidence(
                 "ë‹¤ìŒ ìƒì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3~5ë‹¨ê³„ Phaseë³„ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.\n"
                 "ê° PhaseëŠ” ëª©í‘œ, í•µì‹¬ ì•¡ì…˜(ì±„ë„Â·ì»¨í…ì¸ Â·ì˜¤í¼), ì˜ˆì‚°ë²”ìœ„, ì˜ˆìƒ KPI, ë‹¤ìŒ Phaseë¡œ ë„˜ì–´ê°€ëŠ” ê¸°ì¤€ì„ í¬í•¨í•˜ì„¸ìš”.\n\n"
                 f"- ì—…ì¢…: {info['ì—…ì¢…']}\n"
                 f"- í˜•íƒœ: {info['í”„ëœì°¨ì´ì¦ˆì—¬ë¶€']}\n"
                 f"- ì í¬ì—°ë ¹: {info['ì í¬ì—°ë ¹']}\n"
                 f"- ì£¼ìš” ê³ ê°ì—°ë ¹ëŒ€: {info['ê³ ê°ì—°ë ¹ëŒ€']}\n"
                 f"- ê³ ê°í–‰ë™ íŠ¹ì„±: {info['ê³ ê°í–‰ë™']}\n"
                 "ì‘ë‹µì€ ë¶ˆë¦¿ê³¼ í‘œë¥¼ ì ì ˆíˆ ì„ì–´ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
             )
+        # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
+        kb_opts = st.session_state.get("_kb_opts", {})
+        if kb_opts.get("use"):
+            kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
+            if kb:
+                kb_q = build_kb_query(info, st.session_state.get("pending_question") or "")
+                kb_ev = kb["search"](kb_q, top_k=int(kb_opts.get("topk", 3)))
+                if kb_ev:
+                    prompt += f"\n\n{kb_ev}\n"
@@
-        followup_prompt = build_followup_prompt(
+        followup_prompt = build_followup_prompt(
             user_input,
             st.session_state.get("info", {}),
             strategy_payload,
             raw_strategy,
         )
+        # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
+        kb_opts = st.session_state.get("_kb_opts", {})
+        if kb_opts.get("use"):
+            kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
+            if kb:
+                followup_prompt += "\n\n" + kb["search"](
+                    build_kb_query(st.session_state.get("info", {}), user_input),
+                    top_k=int(kb_opts.get("topk", 3))
+                )

