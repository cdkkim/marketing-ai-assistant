# -*- coding: utf-8 -*-
import os
import io
import csv
import re
import json
import glob
import time
import uuid
import logging
import numpy as np
import streamlit as st
import google.generativeai as genai

# (ì„ íƒ) FAISSê°€ ì—†ìœ¼ë©´ NumPyë¡œ í´ë°±
try:
    import faiss  # type: ignore
except Exception:
    faiss = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) ë¡œê·¸/ê²½ê³  ì–µì œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.getLogger("google.auth").setLevel(logging.ERROR)
os.environ.setdefault("GRPC_VERBOSITY", "ERROR")
os.environ.setdefault("GLOG_minloglevel", "3")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) Gemini API ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    st.warning("âš ï¸ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. (export GOOGLE_API_KEY=...)")

DEFAULT_MODEL = "gemini-2.5-flash"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) í”„ë¡¬í”„íŠ¸ ê°€ì´ë“œ(ì‚¼ì¤‘ë”°ì˜´í‘œë¡œ ì•ˆì „í•˜ê²Œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_EVIDENCE_GUIDE = """
ì¶”ê°€ ì§€ì¹¨:
- ê° ì œì•ˆì—ëŠ” ë°ì´í„° ê·¼ê±°(í‘œ/ì§€í‘œ/ê·œì¹™ ë“±)ë¥¼ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”.
- ê°€ëŠ¥í•œ ê²½ìš° ê°„ë‹¨í•œ í‘œë‚˜ ì§€í‘œ ìˆ˜ì¹˜ë¥¼ í™œìš©í•´ ê·¼ê±°ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ì„¸ìš”.
"""

STRUCTURED_RESPONSE_GUIDE = """
ì‘ë‹µ í˜•ì‹ ì§€ì¹¨(ì¤‘ìš”):
1. ë°˜ë“œì‹œ ë°±í‹±ì´ë‚˜ ì£¼ì„ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
2. JSONì€ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”.
{
  "objective": "ìµœìš°ì„  ë§ˆì¼€íŒ… ëª©í‘œë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½",
  "phase_titles": ["Phase 1: â€¦", "Phase 2: â€¦", "Phase 3: â€¦"],
  "channel_summary": [
    {
      "channel": "ì±„ë„ëª…",
      "phase_title": "ì—°ê²°ëœ Phase ì œëª©",
      "reason": "ì¶”ì²œ ì´ìœ ì™€ ê¸°ëŒ€ íš¨ê³¼",
      "data_evidence": "ê´€ë ¨ ìˆ˜ì¹˜/ê·œì¹™ ë“± ë°ì´í„° ê·¼ê±°"
    }
  ],
  "phases": [
    {
      "title": "Phase 1: â€¦",
      "goal": "êµ¬ì²´ì ì¸ ëª©í‘œ",
      "focus_channels": ["í•µì‹¬ ì±„ë„ 1", "í•µì‹¬ ì±„ë„ 2"],
      "actions": [
        {
          "task": "ì²´í¬ë°•ìŠ¤ì— ë“¤ì–´ê°ˆ ì‹¤í–‰ í•­ëª©",
          "owner": "ë‹´ë‹¹ ì—­í• (ì˜ˆ: ì ì£¼, ìŠ¤íƒœí”„)",
          "supporting_data": "ì„ íƒ) ê´€ë ¨ ë°ì´í„° ê·¼ê±°"
        }
      ],
      "metrics": ["ì„±ê³¼ KPI"],
      "next_phase_criteria": ["ë‹¤ìŒ Phaseë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ ì •ëŸ‰/ì •ì„± ê¸°ì¤€"],
      "data_evidence": ["Phase ì „ëµì„ ë’·ë°›ì¹¨í•˜ëŠ” ê·¼ê±°"]
    }
  ],
  "risks": ["ì£¼ìš” ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ìš”ì•½"],
  "monitoring_cadence": "ëª¨ë‹ˆí„°ë§ ì£¼ê¸°ì™€ ì±…ì„ì"
}
3. PhaseëŠ” ì‹œê°„ ìˆœì„œë¥¼ ì§€í‚¤ê³  Phase 1ì˜ action í•­ëª©ì€ ìµœì†Œ 3ê°œë¥¼ í¬í•¨í•˜ì„¸ìš”.
4. ëª¨ë“  reason, supporting_data, data_evidenceì—ëŠ” ì •ëŸ‰ ìˆ˜ì¹˜ë‚˜ ê·œì¹™ì  ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.
"""

FOLLOWUP_RESPONSE_GUIDE = """
ì‘ë‹µ í˜•ì‹ ì§€ì¹¨(ì¤‘ìš”):
1. ë°˜ë“œì‹œ ë°±í‹±ì´ë‚˜ ì£¼ì„ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.
2. JSONì€ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”.
{
  "summary_points": ["í•µì‹¬ ìš”ì•½ 1", "í•µì‹¬ ìš”ì•½ 2"],
  "detailed_guidance": "ìš”ì•½ì„ í™•ì¥í•˜ëŠ” ìƒì„¸ ì¡°ì–¸",
  "evidence_mentions": ["ê´€ë ¨ ê·¼ê±° ë˜ëŠ” KPI ì–¸ê¸‰"],
  "suggested_question": "ë‹¤ìŒìœ¼ë¡œ ì´ì–´ì§ˆ ê°„ë‹¨í•œ í•œ ë¬¸ì¥ ì§ˆë¬¸"
}
3. summary_pointsëŠ” ìµœëŒ€ 2ê°œ, ê° í•­ëª©ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³  ì¤‘í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
4. evidence_mentionsëŠ” ìµœëŒ€ 3ê°œ ì´ë‚´ì˜ ë¶ˆë¦¿ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ìˆ«ìë‚˜ ì§€í‘œê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
5. detailed_guidanceëŠ” ê¸°ì¡´ ì „ëµì„ ì¬í•´ì„í•˜ë©° ë°ì´í„° ê·¼ê±°ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ë˜ ì‰¬ìš´ ì–´íœ˜ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
6. suggested_questionì€ ì‚¬ìš©ìê°€ ë°”ë¡œ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§§ì€ í›„ì† ì§ˆë¬¸ 1ê°œë§Œ ì œì•ˆí•˜ì„¸ìš”.
"""

def ensure_data_evidence(prompt: str) -> str:
    """í”„ë¡¬í”„íŠ¸ì— ë°ì´í„° ê·¼ê±°/êµ¬ì¡° ê°€ì´ë“œê°€ ì—†ìœ¼ë©´ ì¶”ê°€."""
    updated = prompt.rstrip()
    if "ë°ì´í„° ê·¼ê±°" not in updated:
        updated += "\n\n" + DATA_EVIDENCE_GUIDE.strip()
    if '"phase_titles"' not in updated and "ì‘ë‹µ í˜•ì‹ ì§€ì¹¨(ì¤‘ìš”)" not in updated:
        updated += "\n\n" + STRUCTURED_RESPONSE_GUIDE.strip()
    return updated

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) ê³µí†µ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _extract_bullet_content(text_line: str) -> str | None:
    s = text_line.strip()
    if not s:
        return None
    m1 = re.match(r"^[-\*\u2022]\s*(.+)", s)
    if m1:
        return m1.group(1).strip()
    m2 = re.match(r"^\d+[.)]\s*(.+)", s)
    if m2:
        return m2.group(1).strip()
    return None

def _looks_like_evidence_line(text: str) -> bool:
    """ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ê·¼ê±° ì „ìš© ë¼ì¸ì„ ì‹ë³„."""
    if not text:
        return False
    # ë¶ˆë¦¿/ë²ˆí˜¸ ì ‘ë‘ ì œê±° í›„ ë¹„êµ
    normalized = re.sub(r"^[\-\*\u2022\d\.\)\(\s]+", "", text).strip()
    normalized = normalized.replace(":", " ").strip()
    lower = normalized.lower()
    evidence_prefixes = (
        "ê·¼ê±°",
        "ë°ì´í„° ê·¼ê±°",
        "evidence",
        "ì¦ë¹™",
        "supporting data",
    )
    return any(lower.startswith(prefix) for prefix in evidence_prefixes)

def extract_executive_summary(markdown_text: str, max_points: int = 4):
    """ë¹„êµ¬ì¡° ì‘ë‹µì—ì„œ í•µì‹¬ ë¶ˆë¦¿ì„ ì¶”ì¶œ."""
    lines = markdown_text.splitlines()
    summary = []

    # '# ìš”ì•½' ì„¹ì…˜ ìš°ì„  íƒìƒ‰
    start = None
    for i, line in enumerate(lines):
        if re.match(r"#{1,6}\s*ìš”ì•½", line.strip()):
            start = i; break
    if start is not None:
        for line in lines[start+1:]:
            if line.strip().startswith("#"):
                break
            v = _extract_bullet_content(line)
            if v and not _looks_like_evidence_line(v) and v not in summary:
                summary.append(v)
                if len(summary) >= max_points:
                    break

    # ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ë¶ˆë¦¿ ì¶”ì¶œ
    if not summary:
        for line in lines:
            v = _extract_bullet_content(line)
            if v and not _looks_like_evidence_line(v) and v not in summary:
                summary.append(v)
                if len(summary) >= max_points:
                    break

    # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë¬¸ì¥ ëª‡ ê°œ
    if not summary:
        sentences = re.split(r"(?<=[.!?])\s+", re.sub(r"\s+", " ", markdown_text))
        for s in sentences:
            s = s.strip()
            if s and not _looks_like_evidence_line(s) and s not in summary:
                summary.append(s)
                if len(summary) >= max_points:
                    break
    return summary

def get_action_guidelines_from_session(max_items: int = 2) -> list[str]:
    """ì„¸ì…˜ì— ì €ì¥ëœ ìµœì‹  ì „ëµì—ì„œ ì•¡ì…˜ ê°€ì´ë“œë¥¼ ì¶”ì¶œ."""
    guidelines: list[str] = []
    owner_guidelines: list[str] = []
    owner_keywords = ("ì ì£¼", "ì‚¬ì¥", "ì˜¤ë„ˆ", "ëŒ€í‘œ", "ë§¤ì¥ì£¼", "ìš´ì˜ì")

    def harvest(payload):
        if not payload:
            return
        for phase in payload.get("phases") or []:
            for act in phase.get("actions") or []:
                task = (act.get("task") or "").strip()
                if not task or task in guidelines or task in owner_guidelines:
                    continue
                owner = (act.get("owner") or "").strip()
                if owner and any(k in owner for k in owner_keywords):
                    owner_guidelines.append(task)
                else:
                    guidelines.append(task)
                if len(owner_guidelines) + len(guidelines) >= max_items:
                    return

    payload_candidates = []
    if "latest_strategy" in st.session_state:
        latest = st.session_state.get("latest_strategy") or {}
        payload_candidates.append(latest.get("payload"))
    if "mct_latest_strategy" in st.session_state:
        latest_mct = st.session_state.get("mct_latest_strategy") or {}
        payload_candidates.append(latest_mct.get("payload"))

    for payload in payload_candidates:
        harvest(payload)
        if len(owner_guidelines) + len(guidelines) >= max_items:
            break

    ordered = []
    for task in owner_guidelines:
        if task not in ordered:
            ordered.append(task)
        if len(ordered) >= max_items:
            break
    if len(ordered) < max_items:
        for task in guidelines:
            if task not in ordered:
                ordered.append(task)
            if len(ordered) >= max_items:
                break

    return ordered[:max_items]

def _extract_guidelines_from_text(text: str, max_items: int = 2) -> list[str]:
    """ìƒì„¸ ê°€ì´ë“œ í…ìŠ¤íŠ¸ì—ì„œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì‹¤í–‰ ì§€ì¹¨ ë¬¸ì¥ì„ ì¶”ì¶œ."""
    if not text:
        return []

    keywords = (
        "í•˜ì„¸ìš”", "í•˜ì‹­ì‹œì˜¤", "í•´ë³´ì„¸ìš”", "í•´ ë³´ì„¸ìš”", "í•´ ì£¼ì„¸ìš”", "í•´ì£¼", "ì‹¤í–‰", "ë„ì…",
        "ë§Œë“œ", "ìœ ë„", "ìš´ì˜", "ì œê³µ", "ì¤€ë¹„", "êµ¬ì¶•", "ê°•í™”", "ì—…ë°ì´íŠ¸", "í™•ì‹¤íˆ",
        "í™œìš©", "ë“±ë¡", "ì„¤ì •", "ì ê²€", "ê´€ë¦¬", "ì¦ì •", "í˜œíƒ", "ì¿ í°", "ì´ë²¤íŠ¸",
        "ì„œë¹„ìŠ¤", "ê¸°íš", "ì¶”ì²œ", "ê¶Œì¥", "ìœ ì§€", "í…ŒìŠ¤íŠ¸", "ì—°ê²°", "í™•ì¥", "ìœ ì¹˜"
    )

    candidates: list[str] = []
    for line in text.splitlines():
        cand = _extract_bullet_content(line)
        if cand:
            candidates.append(cand)

    normalized = re.sub(r"\s+", " ", text).strip()
    if normalized:
        for sentence in re.split(r"(?<=[.!?])\s+", normalized):
            sentence = sentence.strip()
            if sentence:
                candidates.append(sentence)

    action_guidelines: list[str] = []
    for cand in candidates:
        if any(keyword in cand for keyword in keywords):
            cleaned = cand.strip()
            if cleaned and cleaned not in action_guidelines:
                action_guidelines.append(cleaned)
            if len(action_guidelines) >= max_items:
                break

    return action_guidelines[:max_items]

def extract_action_guidelines_with_gemini(text: str, max_items: int = 2) -> list[str]:
    """Gemini ëª¨ë¸ë¡œ ì‹¤í–‰ ì§€ì¹¨ì„ ì¶”ì¶œ."""
    if not text or not GOOGLE_API_KEY:
        return []

    def _gather_response_text(response) -> str:
        """ì‘ë‹µ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ íŒŒíŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ."""
        if not response:
            return ""
        pieces: list[str] = []
        candidates = getattr(response, "candidates", None)
        if candidates:
            for cand in candidates:
                content = getattr(cand, "content", None)
                parts = getattr(content, "parts", None) if content else None
                if parts:
                    for part in parts:
                        part_text = getattr(part, "text", None)
                        if part_text:
                            pieces.append(part_text)
            if pieces:
                return "".join(pieces)
        try:
            text_attr = response.text  # type: ignore[attr-defined]
            return text_attr if isinstance(text_attr, str) else ""
        except Exception:
            return ""

    prompt = (
        "ë‹¹ì‹ ì€ ë¡œì»¬ ìƒê¶Œ ì ì£¼ë¥¼ ë•ëŠ” ì‹œë‹ˆì–´ ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ ìƒì„¸ ê°€ì´ë“œë¥¼ ì½ê³ , ë§¤ì¥ ì ì£¼ê°€ ì§ì ‘ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì„¤ë“ë ¥ ìˆëŠ” í–‰ë™ ì œì•ˆì„ "
        f"ìµœëŒ€ {max_items}ê°œ ë„ì¶œí•˜ì„¸ìš”.\n"
        "- ì´ë¯¸ ìƒì„¸ ê°€ì´ë“œì— ìˆëŠ” ë¬¸ì¥ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ë“¬ì–´ë„ ë©ë‹ˆë‹¤.\n"
        "- ë³¸ì‚¬ë‚˜ ì™¸ë¶€ ëŒ€í–‰ì‚¬ê°€ ì•„ë‹Œ ì ì£¼ê°€ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” í•­ëª©ë§Œ ì„ íƒí•˜ì„¸ìš”.\n"
        "- ê° í•­ëª©ì€ ì œì•ˆ/ê¶Œì¥ ì–´ì¡°ë¡œ 1~2ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ì²« ë¬¸ì¥ì—ëŠ” ì‹¤í–‰ í–‰ë™ì„, ë‘ ë²ˆì§¸ ë¬¸ì¥ì—ëŠ” ê¸°ëŒ€ íš¨ê³¼ë‚˜ íŒì„ ë§ë¶™ì´ì„¸ìš”.\n"
        "- ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.\n\n"
        "=== ìƒì„¸ ê°€ì´ë“œ ===\n"
        f"{text.strip()}\n\n"
        "=== ì‘ë‹µ í˜•ì‹ ===\n"
        "{\n"
        '  \"action_guidelines\": [\"ì ì£¼ ì‹¤í–‰ ì§€ì¹¨ 1\", \"ì ì£¼ ì‹¤í–‰ ì§€ì¹¨ 2\"]\n'
        "}\n"
        "- JSON ì™¸ì˜ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "- í•„ìš”í•œ í•­ëª©ë§Œ í¬í•¨í•˜ì„¸ìš”."
    )

    try:
        gmodel = genai.GenerativeModel(DEFAULT_MODEL)
        cfg = {"temperature": 0.2, "max_output_tokens": 65000, "top_p": 0.8, "top_k": 32}
        response = gmodel.generate_content(prompt, generation_config=cfg)
        raw = _gather_response_text(response)
        cleaned = strip_json_artifacts(raw)
        if not cleaned:
            return []
        parsed = json.loads(cleaned)
        if isinstance(parsed, dict):
            values = parsed.get("action_guidelines")
            if isinstance(values, list):
                return [str(v).strip() for v in values[:max_items] if str(v).strip()]
    except json.JSONDecodeError as err:
        logging.info("Gemini action guideline JSON íŒŒì‹± ì‹¤íŒ¨: %s", err)
    except ValueError as err:
        # finish_reasonì´ 2(SAFETY) ë“±ì¼ ë•Œ response.text ì ‘ê·¼ ë“±ì—ì„œ ValueErrorê°€ ë°œìƒ ê°€ëŠ¥
        logging.info("Gemini action guideline í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: %s", err)
    except Exception as err:
        logging.warning("Gemini action guideline ì¶”ì¶œ ì‹¤íŒ¨: %s", err)
    return []

def extract_action_guidelines(text: str, max_items: int = 2) -> list[str]:
    """ìƒì„¸ ê°€ì´ë“œì—ì„œ ì‹¤í–‰ ì§€ì¹¨ì„ ì¶”ì¶œ (Gemini ìš°ì„ , íœ´ë¦¬ìŠ¤í‹± ë³´ì¡°)."""
    guidelines = extract_action_guidelines_with_gemini(text, max_items=max_items)
    if not guidelines:
        guidelines = _extract_guidelines_from_text(text, max_items=max_items)
    return guidelines[:max_items]

def strip_json_artifacts(text: str) -> str:
    cleaned = (text or "").strip().replace("â–Œ", "")
    cleaned = re.sub(r"^```(?:json)?\s*|\s*```$", "", cleaned)
    return cleaned.strip()

def parse_strategy_payload(raw_text: str):
    cand = strip_json_artifacts(raw_text)
    if not cand:
        return None
    try:
        return json.loads(cand)
    except json.JSONDecodeError:
        return None

def parse_followup_payload(raw_text: str):
    cand = strip_json_artifacts(raw_text)
    if not cand:
        return None
    try:
        obj = json.loads(cand)
    except json.JSONDecodeError:
        return None
    return obj if isinstance(obj, dict) else None

INFO_FIELD_ORDER = ["ìƒì ëª…", "ì í¬ì—°ë ¹", "ê³ ê°ì—°ë ¹ëŒ€", "ê³ ê°í–‰ë™"]
BUTTON_HINT = "\n\ní•„ìš”í•œ ì •ë³´ê°€ ì•„ë‹ˆì–´ë„ ì•„ë˜ 'ì´ëŒ€ë¡œ ì§ˆë¬¸' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì§€ê¸ˆ ì •ë³´ë¡œ ë°”ë¡œ ë‹µë³€ì„ ë“œë¦´ê²Œìš”."
DIRECT_RESPONSE_GUIDE = """
ë‹µë³€ ì§€ì¹¨:
- ë¶ˆë¦¿ ëŒ€ì‹  1~2ê°œì˜ ì§§ì€ ë‹¨ë½ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
- ì¤‘í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
- ê°€ëŠ¥í•œ ê²½ìš° ìˆ«ìë‚˜ ê·œì¹™ ê°™ì€ ê·¼ê±°ë¥¼ ë¬¸ì¥ ì•ˆì— ì§ì ‘ ë…¹ì—¬ ì£¼ì„¸ìš”.
- ì‹¤í–‰ ì•„ì´ë””ì–´ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.
- ë§ˆì§€ë§‰ì—ëŠ” `ì¶”ì²œ í›„ì† ì§ˆë¬¸: â€¦` í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì´ì–´ì„œ ë¬¼ì–´ë³¼ ë§Œí•œ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
"""

def get_missing_info_fields(info: dict) -> list:
    missing = [f for f in INFO_FIELD_ORDER if not info.get(f)]
    return missing

def default_suggested_question(info: dict, question: str) -> str:
    text = (question or "").lower()
    if "ë‹¨ê³¨" in text or "ì¬ë°©ë¬¸" in text:
        return "ë‹¨ê³¨ ê³ ê°ì—ê²Œ ì¤„ë§Œí•œ í˜œíƒ ì•„ì´ë””ì–´ë„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"
    if any(k in text for k in ["ë§¤ì¶œ","íŒë§¤","ì‹¤ì "]):
        return "ë§¤ì¶œì„ ë” ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì¶”ê°€ í”„ë¡œëª¨ì…˜ì´ ìˆì„ê¹Œìš”?"
    if any(k in text for k in ["ì‹ ê·œ","ìƒˆ"]):
        return "ì‹ ê·œ ì†ë‹˜ì„ ëŠ˜ë¦¬ë ¤ë©´ ì–´ë–¤ í™ë³´ ì±„ë„ì´ ì¢‹ì„ê¹Œìš”?"
    if any(k in text for k in ["ê´‘ê³ ","í™ë³´","ë§ˆì¼€íŒ…"]):
        return "ê´‘ê³  ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ë¡œ ì¡ìœ¼ë©´ ì¢‹ì„ê¹Œìš”?"
    age = info.get("ê³ ê°ì—°ë ¹ëŒ€","")
    if any(k in age for k in ["30","40"]):
        return "30~40ëŒ€ì—ê²Œ ë°˜ì‘ ì¢‹ì€ ì½˜í…ì¸  ì˜ˆì‹œë¥¼ ë” ì•Œë ¤ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"
    if any(k in age for k in ["50","60"]):
        return "50ëŒ€ ê³ ê°ì´ ì¢‹ì•„í•  ì´ë²¤íŠ¸ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì¶”ì²œí•´ ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"
    return "SNS í™ë³´ ì „ëµë„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"

def parse_direct_answer(answer_text: str, info: dict, question: str) -> tuple[str, str]:
    if not answer_text:
        return "", ""
    guidance = answer_text.strip()
    suggested = ""
    m = re.search(r"ì¶”ì²œ\s*í›„ì†\s*ì§ˆë¬¸\s*:\s*(.+)$", guidance, flags=re.I | re.M)
    if m:
        suggested = m.group(1).strip()
        guidance = guidance[:m.start()].strip()
    if not suggested:
        suggested = default_suggested_question(info, question)
    return guidance, suggested

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) ì‹œê°í™” ì»´í¬ë„ŒíŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_strategy_payload(payload: dict, container, prefix: str = "latest"):
    objective = payload.get("objective")
    if objective:
        container.markdown("### ğŸ¯ Objective")
        container.markdown(objective)

    phase_titles = payload.get("phase_titles") or []
    channel_summary = payload.get("channel_summary") or []
    if channel_summary:
        container.markdown("### ğŸ“Š Recommended Channels & Phase Titles")
        lines = []
        for item in channel_summary:
            channel = item.get("channel","ì±„ë„ ë¯¸ì§€ì •")
            phase_title = item.get("phase_title","Phase ë¯¸ì§€ì •")
            reason = item.get("reason","")
            evidence = item.get("data_evidence","")
            s = f"- **{channel}** â†’ {phase_title}: {reason}"
            if evidence:
                s += f" _(ê·¼ê±°: {evidence})_"
            lines.append(s)
        container.markdown("\n".join(lines))
        if phase_titles:
            container.markdown("**Phase Titles:** " + ", ".join(phase_titles))
    elif phase_titles:
        container.markdown("### ğŸ“‹ Phase Titles")
        container.markdown(", ".join(phase_titles))

    phases = payload.get("phases") or []
    if not phases:
        return

    # Phase 1
    p1 = phases[0]
    c1 = container.container()
    c1.markdown(f"### ğŸš€ {p1.get('title','Phase 1')}")
    if p1.get("goal"):
        c1.markdown(f"**Goal:** {p1['goal']}")
    if p1.get("focus_channels"):
        c1.markdown("**Focus Channels:** " + ", ".join(p1["focus_channels"]))
    actions = p1.get("actions") or []
    if actions:
        c1.markdown("**Action Checklist:**")
        for i, act in enumerate(actions):
            label = act.get("task", f"Action {i+1}")
            owner = act.get("owner")
            support = act.get("supporting_data")
            help_parts = []
            if owner: help_parts.append(f"ë‹´ë‹¹: {owner}")
            if support: help_parts.append(f"ê·¼ê±°: {support}")
            help_txt = " | ".join(help_parts) if help_parts else None
            c1.checkbox(label, key=f"{prefix}_p1_{i}", help=help_txt)
    if p1.get("metrics"):
        c1.markdown("**Metrics:**\n" + "\n".join(f"- {m}" for m in p1["metrics"]))
    if p1.get("next_phase_criteria"):
        c1.markdown("**Criteria To Advance:**\n" + "\n".join(f"- {x}" for x in p1["next_phase_criteria"]))
    if p1.get("data_evidence"):
        c1.markdown("**Data Evidence:**\n" + "\n".join(f"- {e}" for e in p1["data_evidence"]))

    # ë‚˜ë¨¸ì§€ PhaseëŠ” Expander
    for idx, ph in enumerate(phases[1:], start=2):
        ex = container.expander(ph.get("title", f"Phase {idx}"), expanded=False)
        if ph.get("goal"):
            ex.markdown(f"**Goal:** {ph['goal']}")
        if ph.get("focus_channels"):
            ex.markdown("**Focus Channels:** " + ", ".join(ph["focus_channels"]))
        if ph.get("actions"):
            ex.markdown("**Key Actions:**\n" + "\n".join(f"- [ ] {a.get('task','ì‘ì—… ë¯¸ì •')}" for a in ph["actions"]))
        if ph.get("metrics"):
            ex.markdown("**Metrics:**\n" + "\n".join(f"- {m}" for m in ph["metrics"]))
        if ph.get("next_phase_criteria"):
            ex.markdown("**Criteria To Advance:**\n" + "\n".join(f"- {x}" for x in ph["next_phase_criteria"]))
        if ph.get("data_evidence"):
            ex.markdown("**Data Evidence:**\n" + "\n".join(f"- {e}" for e in ph["data_evidence"]))

    risks = payload.get("risks") or []
    cadence = payload.get("monitoring_cadence")
    if risks or cadence:
        container.markdown("### âš ï¸ Risks & Monitoring")
        if risks: container.markdown("\n".join(f"- {r}" for r in risks))
        if cadence: container.markdown(f"**Monitoring Cadence:** {cadence}")

def render_followup_panel(guidance_text: str, evidence_list, suggested_question: str, ui_key: int):
    if not guidance_text:
        return
    st.markdown("### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ")
    summary_points = [
        p for p in extract_executive_summary(guidance_text, max_points=3)
        if not _looks_like_evidence_line(p)
    ]
    action_guidelines = extract_action_guidelines(guidance_text, max_items=2)
    if len(action_guidelines) < 2:
        fallback = get_action_guidelines_from_session(max_items=2)
        for item in fallback:
            if item not in action_guidelines:
                action_guidelines.append(item)
            if len(action_guidelines) >= 2:
                break
    action_guidelines = action_guidelines[:2]
    clean_guidelines: list[str] = []
    for guideline in action_guidelines:
        formatted = guideline.strip()
        if not formatted:
            continue
        if "action guideline" not in formatted.lower():
            formatted = f"{formatted}"
        if formatted not in clean_guidelines:
            clean_guidelines.append(formatted)
        if len(clean_guidelines) >= 2:
            break

    clean_summary: list[str] = []
    for point in summary_points:
        if point and point not in clean_guidelines and point not in clean_summary:
            clean_summary.append(point)

    max_quick_items = 3
    summary_slots = max(0, max_quick_items - len(clean_guidelines))
    display_points: list[str] = clean_summary[:summary_slots]
    if not display_points and not clean_summary and clean_guidelines:
        # ìš”ì•½ì´ ì—†ìœ¼ë©´ í–‰ë™ ì§€ì¹¨ë§Œ ì‚¬ìš©
        display_points = []
    for guideline in clean_guidelines:
        if len(display_points) >= max_quick_items:
            break
        display_points.append(guideline)
    if display_points:
        st.markdown("**ë¹ ë¥¸ ìš”ì•½:**")
        st.markdown("\n".join(f"- {item}" for item in display_points))
    st.markdown(guidance_text)
    if evidence_list:
        st.markdown("**ê·¼ê±°:**")
        st.markdown("\n".join(f"- {item}" for item in (evidence_list or [])))
    first_time = not st.session_state.get("shown_followup_suggestion", False)
    if first_time:
        st.session_state.shown_followup_suggestion = True
        if suggested_question:
            st.markdown(f"**ê°€ëŠ¥í•œ ë‹¤ìŒ ì§ˆë¬¸:** {suggested_question}")
    col1, col2 = st.columns(2)
    suggestion_clicked = False
    if suggested_question:
        suggestion_clicked = col1.button(
            suggested_question, key=f"followup_suggest_{ui_key}",
            use_container_width=True,
            disabled=st.session_state.get("is_generating", False),
        )
    else:
        col1.write("")
    other_clicked = col2.button(
        "ë‹¤ë¥¸ ì§ˆë¬¸ ì…ë ¥", key=f"followup_new_{ui_key}",
        use_container_width=True,
        disabled=st.session_state.get("is_generating", False),
    )
    if suggestion_clicked:
        st.session_state.followup_ui = {}
        st.session_state.auto_followup_question = suggested_question
        st.rerun()
    if other_clicked:
        st.session_state.followup_ui = {}
        st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) personas.json ë¡œë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_personas(path="personas.json"):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        st.error("âš ï¸ personas.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (persona_generator.pyë¡œ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”)")
        return []

personas = load_personas()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) ì—…ì¢…/í”„ëœì°¨ì´ì¦ˆ ì¶”ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BRAND_KEYWORDS_BY_CATEGORY = {
    "ì¹´í˜/ë””ì €íŠ¸": {
        "íŒŒë¦¬","ëšœë ˆ","ë°°ìŠ¤","ë² ìŠ¤","ë² ìŠ¤í‚¨","ë˜í‚¨","í¬ë¦¬ìŠ¤í”¼","íˆ¬ì¸","ì´ë””","ë¹½ë‹¤","ë©”ê°€",
        "ë©”ë¨¸","ë§¤ë¨¸","ì»´í¬","ì»´í¬ì¦ˆ","í• ë¦¬","ìŠ¤íƒ€ë²…","íƒì•¤","ê³µì°¨","ìš”ê±°","ì™€í”ŒëŒ€","ì™€í”Œ","í´ë°”ì…‹",
    },
    "í•œì‹": {
        "êµì´Œ","ë„¤ë„¤","í˜¸ì‹","ë‘˜ë‘˜","ì²˜ê°“","êµ½ë„¤","bbq","bhc","ë§˜ìŠ¤","ë§˜ìŠ¤í„°ì¹˜","ì£ ìŠ¤","ì‹ ì „","êµ­ëŒ€",
        "ëª…ë‘","ë‘ë¼","ë•…ìŠ¤","ëª…ë¥œ","í•˜ë‚¨","ë“±ì´Œ","ë´‰ì¶”","ì›í• ","ë³¸ì£½","ì›í• ë¨¸ë‹ˆ","í•œì´Œ","ë°±ì±„","í•œì†¥","ë°”ë¥´",
    },
    "ì–‘ì‹/ì„¸ê³„ìš”ë¦¬": {"ë„ë¯¸","í”¼ìí—›","íŒŒíŒŒ","íŒŒíŒŒì¡´ìŠ¤","ë¡¯ë°","ë²„ê±°í‚¹","ì¨ë¸Œ","ì„œë¸Œì›¨ì´","ì´ì‚­","í”„ë­","í”„ë­í¬","í”¼ììŠ¤ì¿¨"},
    "ì£¼ì /ì£¼ë¥˜": {"í•œì‹ "},
}
BRAND_KEYWORDS = {kw for kws in BRAND_KEYWORDS_BY_CATEGORY.values() for kw in kws}
AMBIGUOUS_NEGATIVES = {"ì¹´í˜","ì»¤í”¼","ì™•ì‹­","ì„±ìˆ˜","í–‰ë‹¹","ì¢…ë¡œ","ì „ì£¼","ì¶˜ì²œ","ì™€ì¸","ì¹˜í‚¨","í”¼ì","ë¶„ì‹","êµ­ìˆ˜","ì´ˆë°¥","ìŠ¤ì‹œ","ê³±ì°½","ë¼ì§€","í•œìš°","ë§‰ì°½","ìˆ˜ì‚°","ì¶•ì‚°","ë² ì´","ë¸Œë ˆ","ë¸Œë ˆë“œ"}

def _normalize_name(name: str) -> str:
    n = name.lower()
    n = re.sub(r"[\s\*\-\(\)\[\]{}_/\\.|,!?&^%$#@~`+=:;\"']", "", n)
    return n

def classify_hpsn_mct(name: str) -> str:
    normalized = _normalize_name(name or "")
    for category, keywords in BRAND_KEYWORDS_BY_CATEGORY.items():
        if any(k in normalized for k in keywords):
            return category
    nm = (name or "").strip().lower()
    if any(k in nm for k in ["ì¹´í˜","ì»¤í”¼","ë””ì €íŠ¸","ë„ë„ˆì¸ ","ë¹™ìˆ˜","ì™€í”Œ","ë§ˆì¹´ë¡±"]): return "ì¹´í˜/ë””ì €íŠ¸"
    if any(k in nm for k in ["í•œì‹","êµ­ë°¥","ë°±ë°˜","ì°Œê°œ","ê°ìíƒ•","ë¶„ì‹","ì¹˜í‚¨","í•œì •ì‹","ì£½"]): return "í•œì‹"
    if any(k in nm for k in ["ì¼ì‹","ì´ˆë°¥","ëˆê°€ìŠ¤","ë¼ë©˜","ë®ë°¥","ì†Œë°”","ì´ìì¹´ì•¼"]): return "ì¼ì‹"
    if any(k in nm for k in ["ì¤‘ì‹","ì§¬ë½•","ì§œì¥","ë§ˆë¼","í› ê¶ˆ","ë”¤ì„¬"]): return "ì¤‘ì‹"
    if any(k in nm for k in ["ì–‘ì‹","ìŠ¤í…Œì´í¬","í”¼ì","íŒŒìŠ¤íƒ€","í–„ë²„ê±°","ìƒŒë“œìœ„ì¹˜","í† ìŠ¤íŠ¸","ë²„ê±°"]): return "ì–‘ì‹/ì„¸ê³„ìš”ë¦¬"
    if any(k in nm for k in ["ì£¼ì ","í˜¸í”„","ë§¥ì£¼","ì™€ì¸ë°”","ì†Œì£¼","ìš”ë¦¬ì£¼ì ","ì´ìì¹´ì•¼"]): return "ì£¼ì /ì£¼ë¥˜"
    return "ê¸°íƒ€"

def is_franchise(name: str) -> bool:
    n = _normalize_name(name or "")
    if not n:
        return False
    has_branch_marker = "ì " in (name or "")
    hit = any(k in n for k in BRAND_KEYWORDS)
    if hit:
        if any(bad in n for bad in AMBIGUOUS_NEGATIVES):
            short_hits = [k for k in BRAND_KEYWORDS if k in n and len(k) <= 2]
            if short_hits and not has_branch_marker:
                return False
        return True
    return False

def _store_age_label_from_months(months: int) -> str:
    if months <= 12: return "ì‹ ê·œ"
    if months <= 24: return "ì „í™˜ê¸°"
    return "ì˜¤ë˜ëœ"
def _to_pct(value_str):
    try:
        x = float(str(value_str).replace("%","").strip())
        if x <= 1.0:  # 0~1 ìŠ¤ì¼€ì¼ì¼ ë•Œ
            return int(round(x * 100))
        return int(round(min(x, 100)))
    except Exception:
        return None

def render_mct_kpi(perf_score_global: str | float | None, success_label: str | None):
    perf_pct = _to_pct(perf_score_global) if perf_score_global not in (None, "") else None
    label = (success_label or "").strip() or "ë°ì´í„° ì—†ìŒ"

    st.subheader("ğŸ“Š MCT ì„±ê³¼ ìš”ì•½")
    c1, c2 = st.columns(2)
    if perf_pct is not None:
        c1.metric("Perf Score (Global)", f"{perf_pct}%")
    else:
        c1.metric("Perf Score (Global)", "ë°ì´í„° ì—†ìŒ")

    # ê°„ë‹¨í•œ ë±ƒì§€ ëŠë‚Œ
    badge = "âœ… ì„±ê³µ" if label.lower().startswith(("succ","ì„±ê³µ")) else ("âš ï¸ ì£¼ì˜" if "warn" in label.lower() else f"â„¹ï¸ {label}")
    c2.metric("Success Level", badge)

    # ì§„í–‰ë°”(ìˆì„ ë•Œë§Œ)
    if perf_pct is not None:
        st.progress(perf_pct)

def extract_initial_store_info(text: str) -> tuple[dict, str | None]:
    """ë³µí•© ë¬¸ì¥ì—ì„œ ìƒì  ì •ë³´/ì§ˆë¬¸ì„ ë¶„ë¦¬ ì¶”ì¶œ."""
    info_updates, question = {}, None
    if not text: return info_updates, question
    sents = re.split(r"(?<=[?.!])\s+", text.strip())
    info_sents, q_sents = [], []
    for s in sents:
        s = s.strip()
        if not s: continue
        if "?" in s or s.endswith("ê¹Œìš”") or "ì–´ë–»ê²Œ" in s:
            q_sents.append(s)
        else:
            info_sents.append(s)
    if not q_sents and info_sents:
        q_sents.append(info_sents.pop())
    context_text = " ".join(info_sents) if info_sents else text.strip()
    question = " ".join(q_sents).strip() if q_sents else text.strip()

    normalized_no_space = _normalize_name(context_text)
    brand_hits = [kw for kw in BRAND_KEYWORDS if kw in normalized_no_space]
    if brand_hits:
        info_updates["ìƒì ëª…"] = max(brand_hits, key=len)
    else:
        m = re.search(r"([ê°€-í£A-Za-z0-9]+)(?:ì )?(?:ì…ë‹ˆë‹¤|ì´ì—ìš”|ì˜ˆìš”|ì—ìš”)", context_text)
        if m: info_updates["ìƒì ëª…"] = m.group(1)

    age_months = None
    y = re.search(r"(\d+)\s*(?:ë…„|ë…„ì°¨|ë…„ì§¸)", text)
    m = re.search(r"(\d+)\s*(?:ê°œì›”|ë‹¬)", text)
    if y: age_months = int(y.group(1)) * 12
    elif m: age_months = int(m.group(1))
    if age_months is not None:
        info_updates["ì í¬ì—°ë ¹"] = _store_age_label_from_months(age_months)

    if re.search(r"20\s*ëŒ€", text): info_updates["ê³ ê°ì—°ë ¹ëŒ€"] = "20ëŒ€ ì´í•˜ ê³ ê° ì¤‘ì‹¬"
    elif re.search(r"(?:30|40)\s*ëŒ€", text): info_updates["ê³ ê°ì—°ë ¹ëŒ€"] = "30~40ëŒ€ ê³ ê° ì¤‘ì‹¬"
    elif re.search(r"(?:50|60)\s*ëŒ€", text): info_updates["ê³ ê°ì—°ë ¹ëŒ€"] = "50ëŒ€ ì´ìƒ ê³ ê° ì¤‘ì‹¬"

    behaviors = []
    if "ë‹¨ê³¨" in text or "ì¬ë°©ë¬¸" in text: behaviors.append("ì¬ë°©ë¬¸ ê³ ê°")
    if "ì‹ ê·œ" in text or "ìƒˆì†ë‹˜" in text: behaviors.append("ì‹ ê·œ ê³ ê°")
    if "ê±°ì£¼" in text or "ì£¼ë¯¼" in text: behaviors.append("ê±°ì£¼ ê³ ê°")
    if any(k in text for k in ["ì§ì¥","ì˜¤í”¼ìŠ¤","íšŒì‚¬"]): behaviors.append("ì§ì¥ì¸ ê³ ê°")
    if any(k in text for k in ["ìœ ë™","ì§€ë‚˜ê°€ëŠ”","ê´€ê´‘"]): behaviors.append("ìœ ë™ ê³ ê°")
    if behaviors:
        info_updates["ê³ ê°í–‰ë™"] = " + ".join(sorted(set(behaviors)))
    return info_updates, question

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) Gemini ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def stream_gemini(
    prompt,
    model=DEFAULT_MODEL,
    temperature=0.6,
    max_tokens=65535,
    output_placeholder=None,
    status_text="ì „ëµì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤... â³",
    progress_text="AIê°€ ì „ëµì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ“‹",
    success_text="âœ… ì „ëµ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
    error_status_text="ğŸš¨ ì „ëµ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
):
    status_ph = st.empty()
    status_ph.info(status_text)
    try:
        gmodel = genai.GenerativeModel(model)
        cfg = {"temperature": temperature, "max_output_tokens": max_tokens, "top_p": 0.9, "top_k": 40}
        stream = gmodel.generate_content(prompt, generation_config=cfg, stream=True)

        ph = output_placeholder or st.empty()
        ph.info(progress_text)
        full_text = ""
        for event in stream:
            piece = ""
            if getattr(event, "text", None):
                piece = event.text
            elif getattr(event, "candidates", None):
                for c in event.candidates:
                    try:
                        piece += "".join([p.text or "" for p in c.content.parts])
                    except Exception:
                        pass
            if piece:
                full_text += piece

        try:
            stream.resolve()
        except Exception:
            pass

        status_ph.success(success_text)
        if not full_text:
            ph.warning("ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
        return full_text
    except Exception as e:
        status_ph.error(error_status_text)
        st.error(
            "ğŸš¨ Gemini ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
            f"**ì—ëŸ¬ ìœ í˜•**: {type(e).__name__}\n"
            f"**ë©”ì‹œì§€**: {e}\n\n"
            "â€¢ API Key/ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.\n"
            "â€¢ ì¼ì‹œì ì¸ ë„¤íŠ¸ì›Œí¬/ì„œë¹„ìŠ¤ ì´ìŠˆì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) í˜ë¥´ì†Œë‚˜ ë§¤ì¹­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_persona(ì—…ì¢…, í”„ëœì°¨ì´ì¦ˆ, ì í¬ì—°ë ¹="ë¯¸ìƒ", ê³ ê°ì—°ë ¹ëŒ€="ë¯¸ìƒ", ê³ ê°í–‰ë™="ë¯¸ìƒ"):
    for p in personas:
        if p.get("ì—…ì¢…") == ì—…ì¢… and p.get("í”„ëœì°¨ì´ì¦ˆì—¬ë¶€") == í”„ëœì°¨ì´ì¦ˆ:
            return p
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9) ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤(subtitle_summary) RAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=False)
def load_external_kb(dir_path: str, embed_model: str | None = None):
    """subtitle_summary/summary í´ë”ì˜ JSONë“¤ì„ ì„ë² ë”©/ìƒ‰ì¸. ì—†ìœ¼ë©´ None."""
    files = glob.glob(os.path.join(dir_path, "*.json"))
    docs, metas = [], []
    for fp in files:
        try:
            with open(fp, "r", encoding="utf-8") as f:
                data = json.load(f)
            if isinstance(data, dict):
                data = [data]
            for i, d in enumerate(data):
                ctx = d.get("context", {})
                prob = d.get("problem", [])
                sol = d.get("solution", [])
                text = f"ìƒí™©: {ctx}\në¬¸ì œì : {prob}\ní•´ê²°: {sol}"
                docs.append(text)
                metas.append((os.path.basename(fp), i))
        except Exception:
            continue
    if not docs:
        return None

    # ì„ë² ë”© ëª¨ë¸ import (ë¯¸ì„¤ì¹˜ í™˜ê²½ì—ì„œë„ ì•± ì‹¤í–‰ë˜ë„ë¡ ë‚´ë¶€ import)
    try:
        from sentence_transformers import SentenceTransformer  # type: ignore
    except Exception:
        st.warning("sentence-transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì™¸ë¶€ ì§€ì‹ RAGë¥¼ ë¹„í™œì„±í™”í•©ë‹ˆë‹¤.")
        return None

    model_name = os.getenv("EMBED_MODEL_NAME", embed_model or "jhgan/ko-sbert-nli")
    model = SentenceTransformer(model_name)
    doc_emb = model.encode(docs, normalize_embeddings=True)
    doc_emb = np.asarray(doc_emb, dtype=np.float32)

    if faiss is not None:
        index = faiss.IndexFlatIP(doc_emb.shape[1])
        index.add(doc_emb)
    else:
        index = None

    def search(query: str, top_k: int = 3) -> str:
        if not query:
            return ""
        q = model.encode([query], normalize_embeddings=True)
        q = np.asarray(q, dtype=np.float32)
        if index is not None:
            D, I = index.search(q, top_k)
            idxs = I[0]
        else:
            sims = doc_emb @ q[0]
            idxs = np.argsort(-sims)[:top_k]
        lines = ["[ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ ìƒìœ„ ê·¼ê±°]"]
        for rank, idx in enumerate(idxs, 1):
            if idx < 0 or idx >= len(docs):
                continue
            file, j = metas[idx]
            snippet = docs[idx].replace("\n", " ")[:200]
            lines.append(f"{rank}. íŒŒì¼:{file} #{j} | {snippet}")
        return "\n".join(lines)

    return {"search": search}

def build_kb_query(info: dict, extra: str = "") -> str:
    keys = ["ìƒì ëª…", "ì—…ì¢…", "í”„ëœì°¨ì´ì¦ˆì—¬ë¶€", "ì í¬ì—°ë ¹", "ê³ ê°ì—°ë ¹ëŒ€", "ê³ ê°í–‰ë™"]
    parts = [f"{k}:{info[k]}" for k in keys if info.get(k)]
    if extra: parts.append(str(extra))
    return " ".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 10) ENCODED_MCT ì „ìš© (CSV ì—°ë™ + ì „ìš© í™”ë©´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @st.cache_data
# def load_mct_prompts(default_path="store_scores_with_clusterlabel_v2_with_targets_updown.csv", uploaded_file=None):
#     """ENCODED_MCT â†’ {'prompt_str','analysis_prompt_updown'} ë§¤í•‘ ë¡œë“œ."""
#     text = ""
#     src = ""
#     mapping = {}
#     try:
#         if uploaded_file is not None:
#             text = uploaded_file.getvalue().decode("utf-8-sig")
#             src = "uploaded"
#         else:
#             with open(default_path, "r", encoding="utf-8-sig") as f:
#                 text = f.read()
#             src = default_path
#         reader = csv.DictReader(io.StringIO(text))
#         for row in reader:
#             k = (row.get("ENCODED_MCT") or "").strip()
#             if not k: continue
#             mapping[k] = {
#                 "prompt_str": (row.get("prompt_str") or "").strip(),
#                 "analysis_prompt_updown": (row.get("analysis_prompt_updown") or "").strip(),
#             }
#         return mapping, src, None
#     except Exception as e:
#         return {}, src, str(e)
@st.cache_data
def load_mct_prompts(default_path="store_scores_with_clusterlabel_v2_with_targets_updown.csv", uploaded_file=None):
    """
    ENCODED_MCT â†’ {'prompt_str','analysis_prompt_updown','perf_score_global','success_label'} ë§¤í•‘ ë¡œë“œ.
    - ì—…ë¡œë“œ íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œë¥¼ ì‹œë„
    - í—¤ë” ì´ë¦„ì´ ì• ë§¤í•˜ê²Œ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ 'í¬í•¨ í† í°'ìœ¼ë¡œ ìœ ì—° íƒìƒ‰
    """
    import io, csv, re
    text = ""; src = ""; mapping = {}

    def _find_key(row_keys, *tokens):
        # í—¤ë”ì—ì„œ í† í° ì „ë¶€ë¥¼ í¬í•¨í•˜ëŠ” ì²« í‚¤ ë°˜í™˜ (ê³µë°±/ë°‘ì¤„/ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
        for k in row_keys:
            norm = re.sub(r"[^a-z0-9]", "", k.lower())
            if all(t in norm for t in tokens):
                return k
        return None

    try:
        if uploaded_file is not None:
            text = uploaded_file.getvalue().decode("utf-8-sig")
            src = "uploaded"
        else:
            with open(default_path, "r", encoding="utf-8-sig") as f:
                text = f.read()
            src = default_path

        reader = csv.DictReader(io.StringIO(text))
        for row in reader:
            keys = list(row.keys())
            # í•„ìˆ˜ í‚¤ë“¤ ìœ„ì¹˜ ì°¾ê¸°
            key_mct = _find_key(keys, "encoded", "mct") or "ENCODED_MCT"
            if not (row.get(key_mct) or "").strip():
                continue

            key_prompt  = _find_key(keys, "prompt", "str") or "prompt_str"
            key_updown  = _find_key(keys, "analysis", "updown") or "analysis_prompt_updown"

            # ìƒˆë¡œ ì¶”ê°€: perf_score_global / success_label (ì´ë¦„ ë³€í˜•ë„ ì»¤ë²„)
            key_perf    = (_find_key(keys, "perf", "score", "global")
                           or _find_key(keys, "perf", "score")
                           or "perf_score_global")
            key_success = (_find_key(keys, "success", "label")
                           or _find_key(keys, "success")
                           or "success_label")

            mct = (row.get(key_mct) or "").strip()
            mapping[mct] = {
                "prompt_str": (row.get(key_prompt) or "").strip(),
                "analysis_prompt_updown": (row.get(key_updown) or "").strip(),
                "perf_score_global": (row.get(key_perf) or "").strip(),
                "success_label": (row.get(key_success) or "").strip(),
            }
        return mapping, src, None
    except Exception as e:
        return {}, src, str(e)

def build_mct_consult_prompt(info: dict, encoded_mct: str, p_main: str, p_updn: str) -> str:
    name = info.get("ìƒì ëª…") or "-"
    industry = info.get("ì—…ì¢…") or "-"
    franchise = info.get("í”„ëœì°¨ì´ì¦ˆì—¬ë¶€") or "-"
    store_age = info.get("ì í¬ì—°ë ¹") or "-"
    customer_age = info.get("ê³ ê°ì—°ë ¹ëŒ€") or "-"
    behavior = info.get("ê³ ê°í–‰ë™") or "-"
    base = (
        "ë‹¹ì‹ ì€ ì†Œìƒê³µì¸ ì‹ë‹¹/ë¦¬í…Œì¼ì˜ ìš´ì˜Â·ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì‹ í•œì¹´ë“œ ENCODED_MCTë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ë°ì´í„° ê·¼ê±° ì¤‘ì‹¬ì˜ ì‹¤í–‰ ì „ëµ**ë§Œ ì œì‹œí•˜ì„¸ìš”.\n"
        "ëª¨ë“  ì „ëµì€ ê°€ëŠ¥í•œ í•œ **ì •ëŸ‰ ì§€í‘œ(%, %p, ê±´ìˆ˜, ì›)**ë¡œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.\n\n"
        "=== ìƒì  ì¹´ë“œ ===\n"
        f"- ìƒì ëª…: {name}\n"
        f"- ì—…ì¢…: {industry}\n"
        f"- í˜•íƒœ: {franchise}\n"
        f"- ì í¬ì—°ë ¹: {store_age}\n"
        f"- ê³ ê°ì—°ë ¹ëŒ€: {customer_age}\n"
        f"- ê³ ê°í–‰ë™: {behavior}\n\n"
        "=== ENCODED_MCT ===\n"
        f"- ì½”ë“œ: {encoded_mct}\n"
        f"- ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ìš”ì•½:\n{p_main or '-'}\n\n"
        f"- ëª©í‘œ ì—…/ë‹¤ìš´ ì§€ì‹œë¬¸:\n{p_updn or '-'}\n\n"
        "ì •ë ¬ ê·œì¹™(í•„ìˆ˜): ì œì•ˆí•œ ê° ì „ëµì€ ìœ„ 'ëª©í‘œ ì—…/ë‹¤ìš´ ì§€ì‹œë¬¸' ì¤‘ ì–´ë–¤ ì§€í‘œ(â†‘/â†“/ìœ ì§€)ë¥¼ ê²¨ëƒ¥í•˜ëŠ”ì§€ ëª…í™•íˆ ë§¤í•‘í•˜ì„¸ìš”.\n"
        """[ë¹„íŒì  ì˜ê²¬ ì œì‹œ ë°©ì•ˆ]
- detailed_guidanceì˜ ì²« 2ë¬¸ì¥ì€ 'ë°˜ëŒ€ ì‹œê° ìš”ì•½'ìœ¼ë¡œ ì‹œì‘í•˜ë¼(ê°„ê²°í•˜ê²Œ).
- evidence_mentions ì¤‘ 1ê°œëŠ” 'ê·¼ê±°ì˜ í•œê³„'ë¥¼ ì„¤ëª…í•˜ë¼(í‘œë³¸ í¬ê¸°/ê³„ì ˆì„±/ì—­ì¸ê³¼/ì¸¡ì • í¸í–¥ ë“±).
- ì œì•ˆì´ ìœ ì§€ë˜ëŠ” ìµœì†Œ ì¡°ê±´ 1ê°œì™€, ì¤‘ë‹¨í•´ì•¼ í•˜ëŠ” ì‹ í˜¸ 1ê°œë¥¼ ëª…ì‹œí•˜ë¼(ìˆ˜ì¹˜ë‚˜ ê·œì¹™ í˜•íƒœ).
- ì–´ì¡°ëŠ” ì§ì„¤ì ì´ê³  ê²€ì¦ ì¤‘ì‹¬ìœ¼ë¡œ, ê³¼ì¥/ëª¨í˜¸í•œ í‘œí˜„ì„ í”¼í•˜ë¼."""
    )
    return ensure_data_evidence(base)

def render_mct_tab():
    st.header("ğŸ’³ ì‹ í•œì¹´ë“œ ENCODED_MCT ì»¨ì„¤í„´íŠ¸")
    st.markdown(
        "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ ì €ëŠ” **AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸**ì…ë‹ˆë‹¤.\n\n"
        "ìƒì ëª…ì„ ì…ë ¥í•´ì£¼ì‹œë©´ ì—…ì¢…ê³¼ í”„ëœì°¨ì´ì¦ˆ ì—¬ë¶€ë¥¼ ì¶”ì •í•˜ê³ , "
        "ENCODED_MCT(ìƒì  ì„¸ë¶€ ì½”ë“œ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì‹ í•œì¹´ë“œ ì„¸ë¶€ ì •ë³´**ì— ì •ë ¬ëœ ì „ë¬¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n"
        "ì˜ˆ: `êµì´Œì¹˜í‚¨`, `íŒŒë¦¬ë°”ê²Œëœ¨`, `ì¹´í˜í–‰ë‹¹ì `, `ì™•ì‹­ë¦¬ë¼ì§€êµ­ë°¥`"
    )

    # ì „ìš© ì„¸ì…˜ ìƒíƒœ
    if "mct_history" not in st.session_state:
        st.session_state.mct_history = []
    if "mct_info" not in st.session_state:
        st.session_state.mct_info = {}
    if "mct_latest_strategy" not in st.session_state:
        st.session_state.mct_latest_strategy = {}

    with st.expander("ğŸ“„ ENCODED_MCT ì†ŒìŠ¤ CSV (ì„ íƒ ì—…ë¡œë“œ)", expanded=False):
        mct_csv_file = st.file_uploader("í”„ë¡¬í”„íŠ¸ CSV ì—…ë¡œë“œ", type=["csv"], key="mct_csv_uploader")
        st.caption("ê¸°ë³¸ íŒŒì¼ëª…: store_scores_with_clusterlabel_v2_with_targets_updown.csv (í”„ë¡œì íŠ¸ ë£¨íŠ¸)")

    col_a, col_b = st.columns([2, 1])
    with col_a:
        store_name = st.text_input("ìƒì ëª…", key="mct_store_name", placeholder="ì˜ˆ: êµì´Œì¹˜í‚¨ í–‰ë‹¹ì ")
        encoded_mct = st.text_input("ìƒì  ì„¸ë¶€ ì½”ë“œ (ENCODED_MCT)", key="mct_code", placeholder="ì˜ˆ: SEG_KR_05")
    with col_b:
        if store_name:
            guess_industry = classify_hpsn_mct(store_name)
            guess_fr = "í”„ëœì°¨ì´ì¦ˆ" if is_franchise(store_name) else "ê°œì¸ì í¬"
            st.metric("ì˜ˆìƒ ì—…ì¢…", guess_industry)
            st.metric("ì˜ˆìƒ í˜•íƒœ", guess_fr)
    # --- (ì…ë ¥ ë°•ìŠ¤ ë‹¤ìŒ) ENCODED_MCT KPI ë¯¸ë¦¬ë³´ê¸° -----------------
    if encoded_mct:
        mapping_preview, src_preview, err_preview = load_mct_prompts(uploaded_file=mct_csv_file)
        if err_preview:
            st.warning(f"CSV ë¡œë“œ ì‹¤íŒ¨: {err_preview}")
        else:
            row_preview = mapping_preview.get(encoded_mct.strip())
            if row_preview:
                render_mct_kpi(
                    row_preview.get("perf_score_global"),
                    row_preview.get("success_label")
                )
            else:
                st.info("í•´ë‹¹ ENCODED_MCTì— ëŒ€í•œ KPI ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    # ----------------------------------------------------------------
    # ì´ì „ ëŒ€í™” í‘œì‹œ
    for msg in st.session_state.mct_history:
        with st.chat_message(msg["role"]):
            if msg.get("type") == "strategy":
                c = st.container()
                render_strategy_payload(msg.get("data", {}), c, prefix=msg.get("id", "mct_hist"))
            else:
                st.markdown(msg.get("content",""))

    # ìƒì„± ë²„íŠ¼
    generate = st.button("ğŸš€ ì „ë¬¸ ì†”ë£¨ì…˜ ìƒì„±", use_container_width=True, disabled=not encoded_mct)
    if generate:
        info = {
            "ìƒì ëª…": store_name or "",
            "ì—…ì¢…": classify_hpsn_mct(store_name) if store_name else "",
            "í”„ëœì°¨ì´ì¦ˆì—¬ë¶€": ("í”„ëœì°¨ì´ì¦ˆ" if (store_name and is_franchise(store_name)) else "ê°œì¸ì í¬") if store_name else "",
            "ì í¬ì—°ë ¹": st.session_state.mct_info.get("ì í¬ì—°ë ¹", ""),
            "ê³ ê°ì—°ë ¹ëŒ€": st.session_state.mct_info.get("ê³ ê°ì—°ë ¹ëŒ€", ""),
            "ê³ ê°í–‰ë™": st.session_state.mct_info.get("ê³ ê°í–‰ë™", ""),
        }
        st.session_state.mct_info.update({k: v for k, v in info.items() if v})

        # CSV ë¡œë”©
        mapping, src, err = load_mct_prompts(uploaded_file=mct_csv_file)
        p_main, p_updn = "", ""
        if err:
            st.warning(f"CSV ë¡œë“œ ì‹¤íŒ¨: {err}")
        else:
            data = mapping.get((encoded_mct or "").strip())
            if data:
                p_main = data.get("prompt_str", "")
                p_updn = data.get("analysis_prompt_updown", "")
            else:
                st.info("í•´ë‹¹ ENCODED_MCTì— ëŒ€í•œ ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ê°€ ì—†ì–´ ê¸°ë³¸ ë¡œì§ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

        prompt = build_mct_consult_prompt(st.session_state.mct_info, encoded_mct, p_main, p_updn)

        # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
        kb_opts = st.session_state.get("_kb_opts", {})
        if kb_opts.get("use"):
            kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
            if kb:
                kb_q = build_kb_query(st.session_state.mct_info, encoded_mct)
                kb_ev = kb["search"](kb_q, top_k=int(kb_opts.get("topk", 3)))
                if kb_ev:
                    prompt += f"\n\n{kb_ev}\n"

        with st.chat_message("assistant"):
            st.markdown("### ğŸ“ˆ ìƒì„±ëœ ë§ˆì¼€íŒ… ì „ëµ ê²°ê³¼")
            ph = st.empty()
            result = stream_gemini(
                prompt,
                output_placeholder=ph,
                status_text="ENCODED_MCT ì „ë¬¸ ì „ëµì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... â³",
                progress_text="ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ì™€ ìƒì  ì •ë³´ë¥¼ ì •ë ¬ ì¤‘... ğŸ“‹",
                success_text="âœ… ì „ëµ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            if result:
                payload = parse_strategy_payload(result)
                if payload:
                    ph.empty()
                    cid = str(uuid.uuid4())
                    box = st.container()
                    render_strategy_payload(payload, box, prefix=cid)
                    st.session_state.mct_latest_strategy = {"payload": payload, "raw": result}
                    st.session_state.mct_history.append({"role": "assistant", "type": "strategy", "data": payload, "id": cid, "raw": result})
                else:
                    st.markdown(result)
                    st.session_state.mct_latest_strategy = {"payload": None, "raw": result}
                    st.session_state.mct_history.append({"role": "assistant", "content": result})

    # í›„ì† ì§ˆë¬¸
    mct_q = st.chat_input("ENCODED_MCT ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", key="mct_chat_input")
    if mct_q:
        st.session_state.mct_history.append({"role": "user", "content": mct_q})
        latest = st.session_state.get("mct_latest_strategy", {})
        payload = latest.get("payload")
        raw = latest.get("raw", "")

        base_follow = build_followup_prompt(mct_q, st.session_state.mct_info, payload, raw)
        if encoded_mct:
            base_follow += (
                "\n\n[ENCODED_MCT íŒíŠ¸]\n"
                f"- ì½”ë“œ: {encoded_mct}\n"
                "- ìœ„ ì „ëµì˜ ê° í•­ëª©ì„ 'ëª©í‘œ ì§€í‘œ(â†‘/â†“/ìœ ì§€)'ì™€ ê³„ì† ë§¤í•‘í•˜ì„¸ìš”.\n"
            )
        # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
        kb_opts = st.session_state.get("_kb_opts", {})
        if kb_opts.get("use"):
            kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
            print("kb",kb)
            if kb:
                base_follow += "\n\n" + kb["search"](build_kb_query(st.session_state.mct_info, mct_q),
                                                     top_k=int(kb_opts.get("topk", 3)))

        with st.chat_message("assistant"):
            ph2 = st.empty()
            ans = stream_gemini(
                base_follow,
                output_placeholder=ph2,
                status_text="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ’¡",
                progress_text="ê¸°ì¡´ ì „ëµê³¼ ENCODED_MCT ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¥¼ ì¤€ë¹„ ì¤‘... ğŸ§­",
                success_text="âœ… ë‹µë³€ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            if ans:
                parsed = parse_followup_payload(ans)
                if parsed and (parsed.get("summary_points") or parsed.get("detailed_guidance")):
                    pts = (parsed.get("summary_points") or [])[:2]
                    ev = (parsed.get("evidence_mentions") or [])[:3]
                    txt = parsed.get("detailed_guidance", "")
                    guidance = "\n\n".join([s for s in ["\n".join(pts), txt] if s.strip()])
                    render_followup_panel(guidance, ev, (parsed.get("suggested_question") or ""), ui_key=1)
                    st.session_state.mct_history.append({"role": "assistant", "content": guidance})
                else:
                    st.markdown(ans)
                    st.session_state.mct_history.append({"role": "assistant", "content": ans})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 11) ê³µí†µ í”„ë¡¬í”„íŠ¸ ë¹Œë” (ì§ì ‘ ë‹µ/í›„ì†)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_followup_prompt(question: str, info: dict, strategy_payload: dict | None, raw_strategy: str) -> str:
    info_keys = ["ìƒì ëª…","ì—…ì¢…","í”„ëœì°¨ì´ì¦ˆì—¬ë¶€","ì í¬ì—°ë ¹","ê³ ê°ì—°ë ¹ëŒ€","ê³ ê°í–‰ë™"]
    info_lines = [f"- {k}: {info[k]}" for k in info_keys if info.get(k)]
    info_block = "\n".join(info_lines) if info_lines else "- ì¶”ê°€ ìƒì  ì •ë³´ ì—†ìŒ"
    if strategy_payload:
        try:
            strategy_block = json.dumps(strategy_payload, ensure_ascii=False, indent=2)
        except Exception:
            strategy_block = raw_strategy or ""
    else:
        strategy_block = raw_strategy or ""
    prompt = (
        "ë‹¹ì‹ ì€ ì¤‘ì†Œìƒê³µì¸ì„ ë•ëŠ” ì‹œë‹ˆì–´ ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì´ë¯¸ ìƒì„±ëœ ì „ëµ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì „ëµì˜ Phase, ì±„ë„, ì‹¤í–‰ í•­ëª©, ë°ì´í„° ê·¼ê±°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¸ìš©í•˜ê³  í•„ìš” ì‹œ ê°„ë‹¨í•œ ì¶”ê°€ ì¡°ì–¸ì„ ë”í•˜ì„¸ìš”.\n"
        "ìƒˆ ì „ëµì„ ìƒˆë¡œ ì§œì§€ ë§ê³ , ê¸°ì¡´ ì „ëµì„ ì¬í•´ì„í•˜ê±°ë‚˜ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ëª¨ë“  ì„¤ëª…ì€ ì¤‘í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "=== ìƒì  ê¸°ë³¸ ì •ë³´ ===\n"
        f"{info_block}\n\n"
        "=== ê¸°ì¡´ ì „ëµ(JSON) ===\n"
        f"{strategy_block}\n\n"
        "=== ì‚¬ìš©ì ì§ˆë¬¸ ===\n"
        f"{question}\n\n"
        "ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì „ëµ ì •ë³´ë¥¼ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê·¼ê±°ë¡œ í™œìš©í•´ ì¡°ì–¸í•˜ì„¸ìš”.\n"
        "ë°ì´í„° ê·¼ê±° í•­ëª©ì´ë‚˜ KPIê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì–¸ê¸‰í•˜ê±°ë‚˜ ìˆ˜ì¹˜ë¡œ ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”.\n"
        f"{FOLLOWUP_RESPONSE_GUIDE}"
    )
    return prompt

def build_direct_question_prompt(info: dict, question: str, missing_fields=None) -> str:
    missing_fields = missing_fields or []
    info_lines = [f"- {f}: {info[f]}" for f in INFO_FIELD_ORDER if info.get(f)]
    info_block = "\n".join(info_lines) if info_lines else "- ì œê³µëœ ì •ë³´ ì—†ìŒ"
    missing_note = ""
    if missing_fields:
        missing_note = "\n\nì£¼ì˜: ì•„ì§ ë‹¤ìŒ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. " + ", ".join(missing_fields) + "."
    prompt = (
        "ë‹¹ì‹ ì€ ë™ë„¤ ìƒê¶Œì„ ë•ëŠ” ì‹œë‹ˆì–´ ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ ìƒì  ì •ë³´ë¥¼ ì°¸ê³ í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ ì£¼ì„¸ìš”.\n"
        "ë‹µë³€ì€ ë¬¸ë‹¨ í˜•íƒœë¡œ ì‘ì„±í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìˆ˜ì¹˜ë‚˜ ê·œì¹™ ê°™ì€ ê·¼ê±°ë¥¼ ë¬¸ì¥ ì•ˆì— ë…¹ì—¬ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ìƒˆë¡œìš´ ê°€ì •ì„ ë§Œë“¤ê¸°ë³´ë‹¤ëŠ” ì œê³µëœ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.\n\n"
        "=== ìƒì  ì •ë³´ ===\n"
        f"{info_block}\n\n"
        "=== ì‚¬ìš©ì ì§ˆë¬¸ ===\n"
        f"{question}\n"
        f"{missing_note}\n"
        f"{DIRECT_RESPONSE_GUIDE}"
    )
    return prompt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 12) Streamlit UI (ëª¨ë“œ ì „í™˜ + ì‚¬ì´ë“œë°” KB ì˜µì…˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸", layout="wide")
st.title("ğŸ’¬ AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸")

mode = st.sidebar.radio("ëª¨ë“œ", ["ê¸°ì¡´ ìƒë‹´", "ENCODED_MCT ì»¨ì„¤í„´íŠ¸"], index=0)
st.sidebar.divider()
st.sidebar.markdown("**ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤(subtitle_summary) ì‚¬ìš©**")
use_kb = st.sidebar.checkbox("ê·¼ê±° ì£¼ì… ì‚¬ìš©", value=False)
kb_dir = st.sidebar.text_input("KB í´ë” ê²½ë¡œ", "./subtitle_summary/summary", disabled=not use_kb)
kb_topk = st.sidebar.slider("ê·¼ê±° ê°œìˆ˜", 1, 5, 3, disabled=not use_kb)
st.session_state["_kb_opts"] = {"use": use_kb, "dir": kb_dir, "topk": kb_topk}

if mode == "ENCODED_MCT ì»¨ì„¤í„´íŠ¸":
    render_mct_tab()
    st.stop()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 13) ê¸°ì¡´ ìƒë‹´ í”Œë¡œìš° (ì›í˜• ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if st.button("ğŸ”„ ìƒˆ ìƒë‹´ ì‹œì‘"):
    st.session_state.clear()
    st.rerun()

# ìƒíƒœ ì´ˆê¸°í™”
for k, v in {
    "chat_history": [],
    "info": {},
    "initialized": False,
    "latest_strategy": {},
    "followup_ui": {},
    "followup_ui_key": 0,
    "auto_followup_question": None,
    "shown_followup_suggestion": False,
    "pending_question": None,
    "use_pending_question": False,
    "pending_question_button_key": 0,
    "is_generating": False,
}.items():
    st.session_state.setdefault(k, v)

# ì²« ì•ˆë‚´
if not st.session_state.initialized:
    with st.chat_message("assistant"):
        st.markdown(
            "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ ì €ëŠ” **AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸**ì…ë‹ˆë‹¤.\n\n"
            "ìƒì ëª…ì„ ì…ë ¥í•´ì£¼ì‹œë©´ ì—…ì¢…ê³¼ í”„ëœì°¨ì´ì¦ˆ ì—¬ë¶€ë¥¼ ë¶„ì„í•˜ê³ , "
            "ëª‡ ê°€ì§€ ì§ˆë¬¸ì„ í†µí•´ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆë“œë¦´ê²Œìš”.\n\n"
            "ì˜ˆ: `êµì´Œì¹˜í‚¨`, `íŒŒë¦¬ë°”ê²Œëœ¨`, `ì¹´í˜í–‰ë‹¹ì `, `ì™•ì‹­ë¦¬ë¼ì§€êµ­ë°¥`"
        )
    st.session_state.initialized = True

def add_message(role, content=None, **kwargs):
    msg = {"role": role}
    if kwargs.get("type") == "strategy":
        msg["type"] = "strategy"
        msg["data"] = kwargs.get("data", {})
        msg["id"] = kwargs.get("id", str(uuid.uuid4()))
        msg["raw"] = kwargs.get("raw")
    else:
        msg["content"] = content if content is not None else ""
    st.session_state.chat_history.append(msg)

# íˆìŠ¤í† ë¦¬ ë Œë”
for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        if msg.get("type") == "strategy":
            box = st.container()
            render_strategy_payload(msg.get("data", {}), box, prefix=msg.get("id", "history"))
        else:
            st.markdown(msg.get("content",""))

pending_question = st.session_state.get("pending_question")
missing_info = get_missing_info_fields(st.session_state.info)
if pending_question and missing_info:
    with st.container():
        st.info(
            "ì¡°ê¸ˆë§Œ ë” ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤ ì „ëµì„ ë” ì •í™•íˆ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”. "
            "ì§€ê¸ˆ ì •ë³´ë§Œìœ¼ë¡œë„ ë°”ë¡œ ë‹µë³€ì„ ë°›ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
        )
        if st.button(
            "ì´ëŒ€ë¡œ ì§ˆë¬¸",
            key=f"use_question_{st.session_state.pending_question_button_key}",
            use_container_width=True,
            disabled=st.session_state.get("is_generating", False),
        ):
            st.session_state.use_pending_question = True
            st.session_state.auto_followup_question = pending_question
            st.session_state.pending_question_button_key += 1
            st.rerun()

auto_followup_question = st.session_state.pop("auto_followup_question", None)
chat_box_value = st.chat_input("ìƒì ëª…ì„ ì…ë ¥í•˜ê±°ë‚˜ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”...")
user_input = auto_followup_question or chat_box_value

def answer_question_with_current_info(question_text: str):
    info = st.session_state.get("info", {})
    missing_fields = get_missing_info_fields(info)
    prompt = build_direct_question_prompt(info, question_text, missing_fields)

    # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
    kb_opts = st.session_state.get("_kb_opts", {})
    if kb_opts.get("use"):
        kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
        if kb:
            prompt += "\n\n" + kb["search"](build_kb_query(info, question_text), top_k=int(kb_opts.get("topk", 3)))

    with st.chat_message("assistant"):
        placeholder = st.empty()
        st.session_state.is_generating = True
        try:
            answer = stream_gemini(
                prompt,
                output_placeholder=placeholder,
                status_text="ì§ˆë¬¸ì— ëŒ€í•œ ì¡°ì–¸ì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ’¡",
                progress_text="ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ì¡°ì–¸ì„ ëª¨ìœ¼ê³  ìˆìŠµë‹ˆë‹¤... ğŸ§­",
                success_text="âœ… ë‹µë³€ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.",
                error_status_text="ğŸš¨ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            )
        finally:
            st.session_state.is_generating = False
        if answer:
            guidance_text, suggested = parse_direct_answer(answer, info, question_text)
            guidance_text = guidance_text or answer.strip()
            ui_key = st.session_state.get("followup_ui_key", 0) + 1
            st.session_state.followup_ui_key = ui_key
            st.session_state.shown_followup_suggestion = False
            st.session_state.followup_ui = {
                "guidance": guidance_text,
                "evidence": [],
                "suggested_question": suggested,
                "key": ui_key,
            }
            placeholder.empty()
            with st.container():
                render_followup_panel(guidance_text, [], suggested, ui_key)
            log = "### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ\n\n" + guidance_text
            if suggested:
                log += f"\n\n**ê°€ëŠ¥í•œ ë‹¤ìŒ ì§ˆë¬¸:** {suggested}"
            add_message("assistant", log)
            st.session_state.latest_strategy = {"payload": None, "raw": guidance_text}
        else:
            warn = "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ í•´ë³´ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”."
            placeholder.warning(warn)
            add_message("assistant", warn)
            st.session_state.latest_strategy = {"payload": None, "raw": ""}

    if get_missing_info_fields(info):
        st.session_state.pending_question = question_text
    else:
        st.session_state.pending_question = None
    st.session_state.use_pending_question = False
    st.session_state.shown_followup_suggestion = False

# ì…ë ¥ ì²˜ë¦¬
if user_input:
    use_pending = st.session_state.pop("use_pending_question", False)
    st.session_state.followup_ui = {}
    add_message("user", user_input)

    if use_pending:
        answer_question_with_current_info(user_input)
        st.stop()

    # â‘  ìƒì ëª… ìˆ˜ì§‘
    if "ìƒì ëª…" not in st.session_state.info:
        info_updates, detected_q = extract_initial_store_info(user_input)
        name = info_updates.get("ìƒì ëª…") or user_input.strip()
        st.session_state.info["ìƒì ëª…"] = name
        st.session_state.info["ì—…ì¢…"] = classify_hpsn_mct(name)
        st.session_state.info["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"] = "í”„ëœì°¨ì´ì¦ˆ" if is_franchise(name) else "ê°œì¸ì í¬"
        for f in ("ì í¬ì—°ë ¹","ê³ ê°ì—°ë ¹ëŒ€","ê³ ê°í–‰ë™"):
            if info_updates.get(f): st.session_state.info[f] = info_updates[f]
        st.session_state.pending_question = detected_q or user_input
        st.session_state.shown_followup_suggestion = False

        missing = get_missing_info_fields(st.session_state.info)
        if missing:
            nf = missing[0]
            if nf == "ì í¬ì—°ë ¹":
                msg = (
                    f"'{name}'ì€(ëŠ”) **{st.session_state.info['ì—…ì¢…']} ì—…ì¢…**ì´ë©° "
                    f"**{st.session_state.info['í”„ëœì°¨ì´ì¦ˆì—¬ë¶€']}**ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ğŸª\n\n"
                    "ê°œì—… ì‹œê¸°ê°€ ì–¸ì œì¸ê°€ìš”? (ì˜ˆ: 6ê°œì›” ì „, 2ë…„ ì „)"
                )
            elif nf == "ê³ ê°ì—°ë ¹ëŒ€":
                msg = "ì¢‹ì•„ìš” ğŸ‘ ì£¼ìš” ê³ ê°ì¸µì€ ì–´ë–¤ ì—°ë ¹ëŒ€ì¸ê°€ìš”? (20ëŒ€ / 30~40ëŒ€ / 50ëŒ€ ì´ìƒ)"
            else:
                msg = "ë§ˆì§€ë§‰ìœ¼ë¡œ, ê³ ê° ìœ í˜•ì€ ì–´ë–¤ í¸ì¸ê°€ìš”? (ì‰¼í‘œë¡œ êµ¬ë¶„: ì¬ë°©ë¬¸, ì‹ ê·œ, ì§ì¥ì¸, ìœ ë™, ê±°ì£¼)"
            add_message("assistant", msg + BUTTON_HINT)
            st.rerun()
        else:
            st.session_state.pending_question_button_key += 1
            st.session_state.pending_question = st.session_state.pending_question or user_input
            st.session_state.auto_followup_question = st.session_state.pending_question
            st.session_state.use_pending_question = True
            st.rerun()

    # â‘¡ ì í¬ì—°ë ¹
    elif "ì í¬ì—°ë ¹" not in st.session_state.info:
        nums = re.findall(r"\d+", user_input)
        months = int(nums[0]) if nums else 0
        st.session_state.info["ì í¬ì—°ë ¹"] = _store_age_label_from_months(months)
        add_message("assistant", "ì¢‹ì•„ìš” ğŸ‘ ì£¼ìš” ê³ ê°ì¸µì€ ì–´ë–¤ ì—°ë ¹ëŒ€ì¸ê°€ìš”? (20ëŒ€ / 30~40ëŒ€ / 50ëŒ€ ì´ìƒ)" + BUTTON_HINT)
        st.rerun()

    # â‘¢ ê³ ê°ì—°ë ¹ëŒ€
    elif "ê³ ê°ì—°ë ¹ëŒ€" not in st.session_state.info:
        txt = user_input
        if "20" in txt:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "20ëŒ€ ì´í•˜ ê³ ê° ì¤‘ì‹¬"
        elif "30" in txt or "40" in txt:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "30~40ëŒ€ ê³ ê° ì¤‘ì‹¬"
        else:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "50ëŒ€ ì´ìƒ ê³ ê° ì¤‘ì‹¬"
        add_message("assistant", "ë§ˆì§€ë§‰ìœ¼ë¡œ, ê³ ê° ìœ í˜•ì€ ì–´ë–¤ í¸ì¸ê°€ìš”? (ì‰¼í‘œë¡œ êµ¬ë¶„: ì¬ë°©ë¬¸, ì‹ ê·œ, ì§ì¥ì¸, ìœ ë™, ê±°ì£¼)" + BUTTON_HINT)
        st.rerun()

    # â‘£ ê³ ê°í–‰ë™
    elif "ê³ ê°í–‰ë™" not in st.session_state.info:
        parts = re.split(r"[,/+\s]*(?:ë°|ì™€|ê·¸ë¦¬ê³ )?[,/+\s]*", (user_input or "").lower())
        parts = [p for p in parts if p]
        behaviors = []
        for p in parts:
            if "ì¬" in p or "ë‹¨ê³¨" in p: behaviors.append("ì¬ë°©ë¬¸ ê³ ê°")
            if "ì‹ " in p or "ìƒˆ" in p: behaviors.append("ì‹ ê·œ ê³ ê°")
            if "ê±°ì£¼" in p or "ì£¼ë¯¼" in p: behaviors.append("ê±°ì£¼ ê³ ê°")
            if "ì§ì¥" in p or "ì˜¤í”¼ìŠ¤" in p or "íšŒì‚¬" in p: behaviors.append("ì§ì¥ì¸ ê³ ê°")
            if "ìœ ë™" in p or "ì§€ë‚˜" in p or "ê´€ê´‘" in p: behaviors.append("ìœ ë™ ê³ ê°")
        behaviors = list(set(behaviors)) or ["ì¼ë°˜ ê³ ê°"]
        st.session_state.info["ê³ ê°í–‰ë™"] = " + ".join(behaviors)

        # persona ê¸°ë°˜ or fallback ì „ëµ
        info = st.session_state.info
        persona = find_persona(info["ì—…ì¢…"], info["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"], info["ì í¬ì—°ë ¹"], info["ê³ ê°ì—°ë ¹ëŒ€"], info["ê³ ê°í–‰ë™"])
        if persona and persona.get("prompt"):
            prompt = ensure_data_evidence(persona["prompt"])
        else:
            prompt = ensure_data_evidence(
                "ë‹¤ìŒ ìƒì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3~5ë‹¨ê³„ Phaseë³„ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.\n"
                "ê° PhaseëŠ” ëª©í‘œ, í•µì‹¬ ì•¡ì…˜(ì±„ë„Â·ì»¨í…ì¸ Â·ì˜¤í¼), ì˜ˆì‚°ë²”ìœ„, ì˜ˆìƒ KPI, ë‹¤ìŒ Phaseë¡œ ë„˜ì–´ê°€ëŠ” ê¸°ì¤€ì„ í¬í•¨í•˜ì„¸ìš”.\n\n"
                f"- ì—…ì¢…: {info['ì—…ì¢…']}\n"
                f"- í˜•íƒœ: {info['í”„ëœì°¨ì´ì¦ˆì—¬ë¶€']}\n"
                f"- ì í¬ì—°ë ¹: {info['ì í¬ì—°ë ¹']}\n"
                f"- ì£¼ìš” ê³ ê°ì—°ë ¹ëŒ€: {info['ê³ ê°ì—°ë ¹ëŒ€']}\n"
                f"- ê³ ê°í–‰ë™ íŠ¹ì„±: {info['ê³ ê°í–‰ë™']}\n"
                "ì‘ë‹µì€ ë¶ˆë¦¿ê³¼ í‘œë¥¼ ì ì ˆíˆ ì„ì–´ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
            )

        # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
        kb_opts = st.session_state.get("_kb_opts", {})
        if kb_opts.get("use"):
            kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
            if kb:
                kb_q = build_kb_query(info, st.session_state.get("pending_question") or "")
                kb_ev = kb["search"](kb_q, top_k=int(kb_opts.get("topk", 3)))
                if kb_ev:
                    prompt += f"\n\n{kb_ev}\n"

        add_message("assistant", "ì´ì œ AI ìƒë‹´ì‚¬ê°€ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ìƒì„±í•©ë‹ˆë‹¤... â³")
        with st.chat_message("assistant"):
            st.markdown("### ğŸ“ˆ ìƒì„±ëœ ë§ˆì¼€íŒ… ì „ëµ ê²°ê³¼")
            ph = st.empty()
            st.session_state.is_generating = True
            try:
                result = stream_gemini(prompt, output_placeholder=ph)
            finally:
                st.session_state.is_generating = False
            if result:
                payload = parse_strategy_payload(result)
                if payload:
                    mid = str(uuid.uuid4())
                    ph.empty()
                    box = st.container()
                    render_strategy_payload(payload, box, prefix=mid)
                    st.session_state["latest_strategy"] = {"payload": payload, "raw": result}
                    st.session_state.shown_followup_suggestion = False
                    add_message("assistant", type="strategy", data=payload, id=mid, raw=result)
                else:
                    summary_points = extract_executive_summary(result)
                    if summary_points:
                        summary_md = "#### âš¡ í•µì‹¬ ìš”ì•½\n\n" + "\n".join(f"- {p}" for p in summary_points)
                        ph.markdown(summary_md)
                        st.session_state["latest_strategy"] = {"payload": None, "raw": result}
                        st.session_state.shown_followup_suggestion = False
                        add_message("assistant", summary_md)
                    else:
                        fb = "êµ¬ì¡°í™”ëœ ì‘ë‹µì„ í‘œì‹œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”."
                        ph.warning(fb)
                        st.session_state["latest_strategy"] = {"payload": None, "raw": result}
                        st.session_state.shown_followup_suggestion = False
                        add_message("assistant", fb)

    # â‘¤ í›„ì† ì§ˆì˜
    else:
        # ìµœì‹  ì „ëµ/ì›ë¬¸ í™•ë³´
        latest_msg = None
        for item in reversed(st.session_state.get("chat_history", [])):
            if item.get("type") == "strategy":
                latest_msg = item; break
        stored = st.session_state.get("latest_strategy", {})
        strategy_payload = latest_msg.get("data") if latest_msg else stored.get("payload")
        raw_strategy = (latest_msg.get("raw") if latest_msg else None) or stored.get("raw") or ""

        if not (strategy_payload or raw_strategy):
            pending_q = st.session_state.get("pending_question")
            info_state = st.session_state.get("info", {})
            if pending_q and info_state:
                answer_question_with_current_info(pending_q)
                st.rerun()
            fb = "ì•„ì§ ì°¸ê³ í•  ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. ìƒì  ì •ë³´ë¥¼ ì…ë ¥í•´ ë§ì¶¤ ì „ëµì„ ìƒì„±í•˜ê±°ë‚˜ 'ì´ëŒ€ë¡œ ì§ˆë¬¸' ë²„íŠ¼ìœ¼ë¡œ ë°”ë¡œ ì¡°ì–¸ì„ ë°›ì•„ë³´ì„¸ìš”."
            add_message("assistant", fb)
            st.rerun()

        followup_prompt = build_followup_prompt(user_input, st.session_state.get("info", {}), strategy_payload, raw_strategy)

        # ì™¸ë¶€ KB ê·¼ê±° ì£¼ì… (ì˜µì…˜)
        kb_opts = st.session_state.get("_kb_opts", {})
        if kb_opts.get("use"):
            kb = load_external_kb(kb_opts.get("dir", "./subtitle_summary/summary"))
            if kb:
                followup_prompt += "\n\n" + kb["search"](
                    build_kb_query(st.session_state.get("info", {}), user_input),
                    top_k=int(kb_opts.get("topk", 3))
                )

        with st.chat_message("assistant"):
            ph2 = st.empty()
            st.session_state.is_generating = True
            try:
                ans = stream_gemini(
                    followup_prompt,
                    output_placeholder=ph2,
                    status_text="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ’¡",
                    progress_text="ê¸°ì¡´ ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ğŸ§­",
                    success_text="âœ… ë‹µë³€ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    error_status_text="ğŸš¨ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                )
            finally:
                st.session_state.is_generating = False
            if ans:
                parsed = parse_followup_payload(ans)
                if parsed and (parsed.get("summary_points") or parsed.get("detailed_guidance")):
                    pts = (parsed.get("summary_points") or [])[:2]
                    ev = (parsed.get("evidence_mentions") or [])[:3]
                    txt = parsed.get("detailed_guidance", "")
                    guidance = "\n\n".join([s for s in ["\n".join(pts), txt] if s.strip()])
                    guidance = guidance or txt or ans
                    suggested = (parsed.get("suggested_question") or "").strip() or default_suggested_question(st.session_state.get("info", {}), user_input)

                    ui_key = st.session_state.get("followup_ui_key", 0) + 1
                    st.session_state.followup_ui_key = ui_key
                    st.session_state.followup_ui = {
                        "guidance": guidance,
                        "evidence": ev,
                        "suggested_question": suggested,
                        "key": ui_key,
                    }
                    log = "### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ\n\n" + guidance
                    if ev:
                        log += "\n\n**ê·¼ê±°:**\n" + "\n".join(f"- {x}" for x in ev)
                    add_message("assistant", log)
                    ph2.empty()
                    with st.container():
                        render_followup_panel(guidance, ev, suggested, ui_key)
                else:
                    ui_key = st.session_state.get("followup_ui_key", 0) + 1
                    st.session_state.followup_ui_key = ui_key
                    fallback_suggest = default_suggested_question(st.session_state.get("info", {}), user_input)
                    st.session_state.followup_ui = {
                        "guidance": ans,
                        "evidence": [],
                        "suggested_question": fallback_suggest,
                        "key": ui_key,
                    }
                    clean = (ans or "").strip()
                    if clean:
                        log = "### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ\n\n" + clean
                        if fallback_suggest:
                            log += f"\n\n**ê°€ëŠ¥í•œ ë‹¤ìŒ ì§ˆë¬¸:** {fallback_suggest}"
                        add_message("assistant", log)
                    ph2.empty()
                    with st.container():
                        render_followup_panel(ans, [], fallback_suggest, ui_key)
            else:
                ph2.warning("ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ í•´ë³´ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”.")
else:
    # ê¸°ì¡´ ì„¸ì…˜ì˜ followup íŒ¨ë„ ì¬í‘œì‹œ
    ui = st.session_state.get("followup_ui", {})
    if ui.get("guidance"):
        with st.chat_message("assistant"):
            render_followup_panel(ui.get("guidance",""), ui.get("evidence", []), ui.get("suggested_question",""), ui.get("key", 0))
