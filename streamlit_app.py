import os
import re
import json
import logging
import time
import streamlit as st
import google.generativeai as genai

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ë¡œê·¸ ë° ê²½ê³  ì–µì œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.getLogger("google.auth").setLevel(logging.ERROR)
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "3"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Gemini API ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    st.warning("âš ï¸ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit secretsì— ì¶”ê°€í•˜ì„¸ìš”.")

DEFAULT_MODEL = "gemini-2.5-flash"
DATA_EVIDENCE_GUIDE = (
    "\n\nì¶”ê°€ ì§€ì¹¨:\n"
    "- ê° ì œì•ˆì—ëŠ” ë°ì´í„° ê·¼ê±°(í‘œ/ì§€í‘œ/ê·œì¹™ ë“±)ë¥¼ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”.\n"
    "- ê°€ëŠ¥í•œ ê²½ìš° ê°„ë‹¨í•œ í‘œë‚˜ ì§€í‘œ ìˆ˜ì¹˜ë¥¼ í™œìš©í•´ ê·¼ê±°ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ì„¸ìš”."
)


def ensure_data_evidence(prompt: str) -> str:
    """í”„ë¡¬í”„íŠ¸ì— ë°ì´í„° ê·¼ê±° ì§€ì¹¨ì´ ì—†ìœ¼ë©´ ì¶”ê°€."""
    if "ë°ì´í„° ê·¼ê±°" in prompt:
        return prompt
    return prompt.rstrip() + DATA_EVIDENCE_GUIDE


def extract_executive_summary(markdown_text: str, max_points: int = 4):
    """ìƒì„±ëœ ì „ëµ ë³¸ë¬¸ì—ì„œ ìš”ì•½ ì„¹ì…˜ì˜ í•µì‹¬ ë¶ˆë¦¿ì„ ì¶”ì¶œ."""
    lines = markdown_text.splitlines()
    summary_lines = []

    def clean_bullet(line):
        stripped = line.strip()
        if not stripped:
            return None
        bullet_match = re.match(r"^[-\*\u2022]\s*(.+)", stripped)
        if bullet_match:
            return bullet_match.group(1).strip()
        numbered_match = re.match(r"^\d+[.)]\s*(.+)", stripped)
        if numbered_match:
            return numbered_match.group(1).strip()
        return None

    heading_pattern = re.compile(r"#{1,6}\s*ìš”ì•½")
    start_idx = next(
        (idx for idx, line in enumerate(lines) if heading_pattern.match(line.strip())),
        None,
    )

    if start_idx is not None:
        for line in lines[start_idx + 1 :]:
            stripped = line.strip()
            if stripped.startswith("#"):
                break
            cleaned = clean_bullet(stripped)
            if cleaned:
                summary_lines.append(cleaned)
            if len(summary_lines) >= max_points:
                break

    if not summary_lines:
        for line in lines:
            cleaned = clean_bullet(line)
            if cleaned:
                summary_lines.append(cleaned)
            if len(summary_lines) >= max_points:
                break

    if not summary_lines:
        collapsed = re.sub(r"\s+", " ", markdown_text)
        sentences = re.split(r"(?<=[.!?])\s", collapsed)
        for sentence in sentences:
            cleaned = sentence.strip()
            if cleaned:
                summary_lines.append(cleaned)
            if len(summary_lines) >= max_points:
                break

    return summary_lines

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Persona ë°ì´í„° ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_personas(path="personas.json"):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        st.error("âš ï¸ personas.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. prompt_generator.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return []

personas = load_personas()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì—…ì¢… ë¶„ë¥˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def classify_hpsn_mct(name: str) -> str:
    nm = name.strip().lower()
    if any(k in nm for k in ["ì¹´í˜", "ì»¤í”¼", "ë””ì €íŠ¸", "ë„ë„ˆì¸ ", "ë¹™ìˆ˜", "ì™€í”Œ", "ë§ˆì¹´ë¡±"]):
        return "ì¹´í˜/ë””ì €íŠ¸"
    if any(k in nm for k in ["í•œì‹", "êµ­ë°¥", "ë°±ë°˜", "ì°Œê°œ", "ê°ìíƒ•", "ë¶„ì‹", "ì¹˜í‚¨", "í•œì •ì‹", "ì£½"]):
        return "í•œì‹"
    if any(k in nm for k in ["ì¼ì‹", "ì´ˆë°¥", "ëˆê°€ìŠ¤", "ë¼ë©˜", "ë®ë°¥", "ì†Œë°”", "ì´ìì¹´ì•¼"]):
        return "ì¼ì‹"
    if any(k in nm for k in ["ì¤‘ì‹", "ì§¬ë½•", "ì§œì¥", "ë§ˆë¼", "í› ê¶ˆ", "ë”¤ì„¬"]):
        return "ì¤‘ì‹"
    if any(k in nm for k in ["ì–‘ì‹", "ìŠ¤í…Œì´í¬", "í”¼ì", "íŒŒìŠ¤íƒ€", "í–„ë²„ê±°", "ìƒŒë“œìœ„ì¹˜", "í† ìŠ¤íŠ¸", "ë²„ê±°"]):
        return "ì–‘ì‹/ì„¸ê³„ìš”ë¦¬"
    if any(k in nm for k in ["ì£¼ì ", "í˜¸í”„", "ë§¥ì£¼", "ì™€ì¸ë°”", "ì†Œì£¼", "ìš”ë¦¬ì£¼ì ", "ì´ìì¹´ì•¼"]):
        return "ì£¼ì /ì£¼ë¥˜"
    return "ê¸°íƒ€"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. í”„ëœì°¨ì´ì¦ˆ íŒë³„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BRAND_KEYWORDS = {
    "íŒŒë¦¬","ëšœë ˆ","ë°°ìŠ¤","ë˜í‚¨","íˆ¬ì¸","ì´ë””","ë¹½ë‹¤","ë©”ê°€","ì»´í¬","í• ë¦¬",
    "ìŠ¤íƒ€ë²…","íƒì•¤","ê³µì°¨","ìš”ê±°","ì™€í”Œ","êµì´Œ","ë„¤ë„¤","í˜¸ì‹","ë‘˜ë‘˜","ì²˜ê°“",
    "êµ½ë„¤","bbq","bhc","ë§˜ìŠ¤","ì£ ìŠ¤","ì‹ ì „","ëª…ë‘","ë‘ë¼","ë•…ìŠ¤",
    "ë„ë¯¸","íŒŒíŒŒ","ë¡¯ë°","ë²„ê±°í‚¹","ì¨ë¸Œ","ì´ì‚­","ëª…ë¥œ","í•˜ë‚¨","í•œì‹ ",
    "ë“±ì´Œ","ë´‰ì¶”","ì›í• ","ë³¸ì£½","í•œì´Œ","ë°±ì±„","í”„ë­","ë°”ë¥´","í•œì†¥","ë² ìŠ¤"
}
AMBIGUOUS_NEGATIVES = {
    "ì¹´í˜","ì»¤í”¼","ì™•ì‹­","ì„±ìˆ˜","í–‰ë‹¹","ì¢…ë¡œ","ì „ì£¼","ì¶˜ì²œ","ì™€ì¸","ì¹˜í‚¨","í”¼ì",
    "ë¶„ì‹","êµ­ìˆ˜","ì´ˆë°¥","ê³±ì°½","ë¼ì§€","í•œìš°"
}

def _normalize_name(name: str) -> str:
    n = name.lower()
    n = re.sub(r"[\s\*\-\(\)\[\]{}_/\\.|,!?&^%$#@~`+=:;\"']", "", n)
    return n

def is_franchise(name: str) -> bool:
    n = _normalize_name(name)
    if not n:
        return False
    has_branch_marker = "ì " in name
    hit = any(k in n for k in BRAND_KEYWORDS)
    if hit:
        if any(bad in n for bad in AMBIGUOUS_NEGATIVES):
            short_hits = [k for k in BRAND_KEYWORDS if k in n and len(k) <= 2]
            if short_hits and not has_branch_marker:
                return False
        return True
    return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Gemini Streaming í˜¸ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MODEL = "gemini-2.5-flash"

# https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash
def stream_gemini(
    prompt,
    model=DEFAULT_MODEL,
    temperature=0.6,
    max_tokens=65535,
    output_placeholder=None,
):
    """ì•ˆì •ì ì¸ ìŠ¤íŠ¸ë¦¬ë° + ì™„ë£Œì‚¬ìœ  ì ê²€ + ì¹œì ˆí•œ ì—ëŸ¬"""
    status_placeholder = st.empty()
    status_placeholder.info("ì „ëµì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤... â³")

    status_messages = [
        "1/4 ì‹œì¥ ë° ê²½ìŸ ë°ì´í„°ë¥¼ ê²€í† í•˜ê³  ìˆì–´ìš”...",
        "2/4 ë”± ë§ëŠ” ë§ˆì¼€íŒ… ì±„ë„ì„ ì¡°ì‚¬í•˜ê³  ìˆì–´ìš”...",
        "3/4 ì‹¤í–‰ ê°€ëŠ¥í•œ ì „ëµ ì•„ì´ë””ì–´ë¥¼ ì¡°í•©í•˜ëŠ” ì¤‘ì´ì—ìš”...",
        "4/4 ì „ë‹¬í•  ë‚´ìš©ì„ ì •ëˆí•˜ê³  ìˆì–´ìš”...",
    ]
    step_interval = 2.0
    step_state = {"idx": 0, "next_time": time.time() + step_interval}

    try:
        gmodel = genai.GenerativeModel(model)
        cfg = {
            "temperature": temperature,
            "max_output_tokens": max_tokens,
            "top_p": 0.9,
            "top_k": 40,
        }

        stream = gmodel.generate_content(prompt, generation_config=cfg, stream=True)

        placeholder = output_placeholder or st.empty()
        full_text = ""

        # 1) ìŠ¤íŠ¸ë¦¬ë° ìˆ˜ì§‘ (chunk.textê°€ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ candidatesë„ í™•ì¸)
        for event in stream:
            now = time.time()
            if step_state["idx"] < len(status_messages) and now >= step_state["next_time"]:
                status_placeholder.info(
                    "ì „ëµì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤... â³\n\n"
                    f"{status_messages[step_state['idx']]}"
                )
                step_state["idx"] += 1
                step_state["next_time"] = now + step_interval

            piece = ""
            if getattr(event, "text", None):
                piece = event.text
            elif getattr(event, "candidates", None):
                # ì¼ë¶€ ì´ë²¤íŠ¸ëŠ” delta í˜•íƒœë¡œ ë“¤ì–´ì™€ì„œ textê°€ ë¹„ì–´ ìˆìŒ
                for c in event.candidates:
                    # ê° candidateì˜ contentì—ì„œ ì¶”ê°€ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
                    try:
                        piece += "".join([p.text or "" for p in c.content.parts])
                    except Exception:
                        pass

            if piece:
                full_text += piece
                # íƒ€ì´í•‘ ì»¤ì„œ í‘œì‹œ
                placeholder.markdown(full_text + "â–Œ")

        # 2) ìµœì¢… í•´ì„ (finish_reason/blocked ì—¬ë¶€ í™•ì¸)
        try:
            stream.resolve()  # ìµœì¢… ìƒíƒœ/ë©”íƒ€ í™•ë³´
        except Exception:
            # resolveì—ì„œ ì˜¤ë¥˜ê°€ ë‚˜ë„ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ê³„ì† ì§„í–‰
            pass

        placeholder.markdown(full_text or "_ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤._")

        # finish_reason/blocked ì•ˆë‚´
        try:
            cand0 = stream.candidates[0]
            fr = getattr(cand0, "finish_reason", None)
            block = getattr(cand0, "safety_ratings", None)
        except Exception:
            fr, block = None, None

        # 3) ì˜ë¦¼/ì°¨ë‹¨ ì•ˆë‚´ + ì´ì–´ì“°ê¸° ë²„íŠ¼
        if fr == "MAX_TOKENS":
            st.info("â„¹ï¸ ì‘ë‹µì´ ê¸¸ì–´ ì¤‘ê°„ì— ì˜ë ¸ì–´ìš”. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì´ì–´ì„œ ìƒì„±í•  ìˆ˜ ìˆì–´ìš”.")
            if st.button("â• ì´ì–´ì„œ ë” ìƒì„±"):
                continue_from(full_text, prompt, gmodel, cfg)
        elif fr == "SAFETY":
            st.warning("âš ï¸ ì•ˆì „ í•„í„°ë¡œ ì¼ë¶€ ë‚´ìš©ì´ ìˆ¨ê²¨ì¡Œì„ ìˆ˜ ìˆì–´ìš”. í‘œí˜„ì„ ë‹¤ë“¬ì–´ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")

        status_placeholder.success("âœ… ì „ëµ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return full_text

    except Exception as e:
        status_placeholder.error("ğŸš¨ ì „ëµ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        st.error(
            "ğŸš¨ Gemini ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
            f"**ì—ëŸ¬ ìœ í˜•**: {type(e).__name__}\n"
            f"**ë©”ì‹œì§€**: {e}\n\n"
            "â€¢ API Key/ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.\n"
            "â€¢ ì¼ì‹œì ì¸ ë„¤íŠ¸ì›Œí¬/ì„œë¹„ìŠ¤ ì´ìŠˆì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )
        return None


def continue_from(previous_text: str, original_prompt: str, gmodel, cfg):
    """
    MAX_TOKENSë¡œ ì˜ë ¸ì„ ë•Œ ì´ì–´ì“°ê¸°. ì›ë³¸ ë¬¸ë§¥ì„ ê°„ë‹¨íˆ ìš”ì•½Â·ë³µì›í•´ ì—°ì†ì„± ìœ ì§€.
    """
    followup_prompt = (
        "ì•„ë˜ ì´ˆì•ˆì˜ ì´ì–´ì§€ëŠ” ë‚´ìš©ì„ ê°™ì€ í†¤/ì„œì‹ìœ¼ë¡œ ê³„ì† ì‘ì„±í•˜ì„¸ìš”. "
        "ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ Phase ë‚˜ë¨¸ì§€ì™€ KPI, ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ë§ˆì € ì±„ì›Œì£¼ì„¸ìš”.\n\n"
        "=== ì§€ê¸ˆê¹Œì§€ ìƒì„±ëœ ì´ˆì•ˆ ===\n"
        f"{previous_text}\n"
        "=== ì›ë˜ì˜ ìš”êµ¬ì‚¬í•­ ===\n"
        f"{original_prompt}\n"
    )

    try:
        stream2 = gmodel.generate_content(followup_prompt, generation_config=cfg, stream=True)
        placeholder = st.empty()
        full2 = ""
        for ev in stream2:
            if getattr(ev, "text", None):
                full2 += ev.text
                placeholder.markdown(full2 + "â–Œ")
        placeholder.markdown(full2)
        st.session_state.chat_history.append({"role": "assistant", "content": full2})
    except Exception as e:
        st.error(
            "ğŸš¨ ì´ì–´ì“°ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
            f"**ì—ëŸ¬ ìœ í˜•**: {type(e).__name__}\n"
            f"**ë©”ì‹œì§€**: {e}"
        )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. í˜ë¥´ì†Œë‚˜ ë§¤ì¹­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_persona(ì—…ì¢…, í”„ëœì°¨ì´ì¦ˆ, ì í¬ì—°ë ¹="ë¯¸ìƒ", ê³ ê°ì—°ë ¹ëŒ€="ë¯¸ìƒ", ê³ ê°í–‰ë™="ë¯¸ìƒ"):
    for p in personas:
        if p["ì—…ì¢…"] == ì—…ì¢… and p["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"] == í”„ëœì°¨ì´ì¦ˆ:
            return p
    return None  # None ê·¸ëŒ€ë¡œ ë°˜í™˜

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. Streamlit UI ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸", layout="wide")
st.title("ğŸ’¬ AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸")

if st.button("ğŸ”„ ìƒˆ ìƒë‹´ ì‹œì‘"):
    st.session_state.clear()
    st.rerun()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "info" not in st.session_state:
    st.session_state.info = {}
if "initialized" not in st.session_state:
    st.session_state.initialized = False

if not st.session_state.initialized:
    with st.chat_message("assistant"):
        st.markdown(
            "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ ì €ëŠ” **AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸**ì…ë‹ˆë‹¤.\n\n"
            "ìƒì ëª…ì„ ì…ë ¥í•´ì£¼ì‹œë©´ ì—…ì¢…ê³¼ í”„ëœì°¨ì´ì¦ˆ ì—¬ë¶€ë¥¼ ë¶„ì„í•˜ê³ , "
            "ëª‡ ê°€ì§€ ì§ˆë¬¸ì„ í†µí•´ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆë“œë¦´ê²Œìš”.\n\n"
            "ì˜ˆ: `êµì´Œì¹˜í‚¨`, `íŒŒë¦¬ë°”ê²Œëœ¨`, `ì¹´í˜í–‰ë‹¹ì `, `ì™•ì‹­ë¦¬ë¼ì§€êµ­ë°¥`"
        )
    st.session_state.initialized = True

for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("ìƒì ëª…ì„ ì…ë ¥í•˜ê±°ë‚˜ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”...")

def add_message(role, content):
    st.session_state.chat_history.append({"role": role, "content": content})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. ëŒ€í™” ë¡œì§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if user_input:
    add_message("user", user_input)

    # â‘  ìƒì ëª…
    if "ìƒì ëª…" not in st.session_state.info:
        name = user_input.strip()
        st.session_state.info["ìƒì ëª…"] = name
        st.session_state.info["ì—…ì¢…"] = classify_hpsn_mct(name)
        st.session_state.info["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"] = "í”„ëœì°¨ì´ì¦ˆ" if is_franchise(name) else "ê°œì¸ì í¬"

        add_message(
            "assistant",
            f"'{name}'ì€(ëŠ”) **{st.session_state.info['ì—…ì¢…']} ì—…ì¢…**ì´ë©° "
            f"**{st.session_state.info['í”„ëœì°¨ì´ì¦ˆì—¬ë¶€']}**ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ğŸª\n\n"
            "ê°œì—… ì‹œê¸°ê°€ ì–¸ì œì¸ê°€ìš”? (ì˜ˆ: 6ê°œì›” ì „, 2ë…„ ì „)"
        )
        st.rerun()

    # â‘¡ ê°œì—… ì‹œê¸°
    elif "ì í¬ì—°ë ¹" not in st.session_state.info:
        months = re.findall(r"\d+", user_input)
        months = int(months[0]) if months else 0
        if months <= 12:
            st.session_state.info["ì í¬ì—°ë ¹"] = "ì‹ ê·œ"
        elif months <= 24:
            st.session_state.info["ì í¬ì—°ë ¹"] = "ì „í™˜ê¸°"
        else:
            st.session_state.info["ì í¬ì—°ë ¹"] = "ì˜¤ë˜ëœ"

        add_message("assistant", "ì¢‹ì•„ìš” ğŸ‘ ì£¼ìš” ê³ ê°ì¸µì€ ì–´ë–¤ ì—°ë ¹ëŒ€ì¸ê°€ìš”? (20ëŒ€ / 30~40ëŒ€ / 50ëŒ€ ì´ìƒ)")
        st.rerun()

    # â‘¢ ê³ ê° ì—°ë ¹ëŒ€
    elif "ê³ ê°ì—°ë ¹ëŒ€" not in st.session_state.info:
        txt = user_input
        if "20" in txt:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "20ëŒ€ ì´í•˜ ê³ ê° ì¤‘ì‹¬"
        elif "30" in txt or "40" in txt:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "30~40ëŒ€ ê³ ê° ì¤‘ì‹¬"
        else:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "50ëŒ€ ì´ìƒ ê³ ê° ì¤‘ì‹¬"

        add_message("assistant", "ë§ˆì§€ë§‰ìœ¼ë¡œ, ê³ ê° ìœ í˜•ì€ ì–´ë–¤ í¸ì¸ê°€ìš”? (ì‰¼í‘œë¡œ êµ¬ë¶„ ê°€ëŠ¥: ì¬ë°©ë¬¸, ì‹ ê·œ, ì§ì¥ì¸, ìœ ë™, ê±°ì£¼)")
        st.rerun()

    # â‘£ ê³ ê°í–‰ë™ (ë‹¤ì¤‘ ì…ë ¥ ìœ ì—° íŒŒì‹±)
    elif "ê³ ê°í–‰ë™" not in st.session_state.info:
        txt = user_input.lower()
        parts = re.split(r"[,/+\s]*(?:ë°|ì™€|ê·¸ë¦¬ê³ )?[,/+\s]*", txt)
        parts = [p for p in parts if p]

        behaviors = []
        for p in parts:
            if "ì¬" in p or "ë‹¨ê³¨" in p:
                behaviors.append("ì¬ë°©ë¬¸ ê³ ê°")
            if "ì‹ " in p or "ìƒˆ" in p:
                behaviors.append("ì‹ ê·œ ê³ ê°")
            if "ê±°ì£¼" in p or "ì£¼ë¯¼" in p:
                behaviors.append("ê±°ì£¼ ê³ ê°")
            if "ì§ì¥" in p or "ì˜¤í”¼ìŠ¤" in p or "íšŒì‚¬" in p:
                behaviors.append("ì§ì¥ì¸ ê³ ê°")
            if "ìœ ë™" in p or "ì§€ë‚˜" in p or "ê´€ê´‘" in p:
                behaviors.append("ìœ ë™ ê³ ê°")

        behaviors = list(set(behaviors)) or ["ì¼ë°˜ ê³ ê°"]
        st.session_state.info["ê³ ê°í–‰ë™"] = " + ".join(behaviors)

        # ... ê³ ê°í–‰ë™ê¹Œì§€ ìˆ˜ì§‘ëœ ë’¤:
        info = st.session_state.info
        persona = find_persona(
            info["ì—…ì¢…"], info["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"],
            info["ì í¬ì—°ë ¹"], info["ê³ ê°ì—°ë ¹ëŒ€"], info["ê³ ê°í–‰ë™"]
        )

        # â‘  persona prompt ë˜ëŠ” â‘¡ fallback prompt
        if persona and "prompt" in persona:
            prompt = ensure_data_evidence(persona["prompt"])
        else:
            prompt = ensure_data_evidence(
                "ë‹¤ìŒ ìƒì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3~5ë‹¨ê³„ Phaseë³„ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.\n"
                "ê° PhaseëŠ” ëª©í‘œ, í•µì‹¬ ì•¡ì…˜(ì±„ë„Â·ì»¨í…ì¸ Â·ì˜¤í¼), ì˜ˆì‚°ë²”ìœ„, ì˜ˆìƒ KPI, ë‹¤ìŒ Phaseë¡œ ë„˜ì–´ê°€ëŠ” ê¸°ì¤€ì„ í¬í•¨í•˜ì„¸ìš”.\n\n"
                f"- ì—…ì¢…: {info['ì—…ì¢…']}\n"
                f"- í˜•íƒœ: {info['í”„ëœì°¨ì´ì¦ˆì—¬ë¶€']}\n"
                f"- ì í¬ì—°ë ¹: {info['ì í¬ì—°ë ¹']}\n"
                f"- ì£¼ìš” ê³ ê°ì—°ë ¹ëŒ€: {info['ê³ ê°ì—°ë ¹ëŒ€']}\n"
                f"- ê³ ê°í–‰ë™ íŠ¹ì„±: {info['ê³ ê°í–‰ë™']}\n"
                "ì‘ë‹µì€ ë¶ˆë¦¿ê³¼ í‘œë¥¼ ì ì ˆíˆ ì„ì–´ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
            )

        #with st.expander("ğŸ“œ í”„ë¡¬í”„íŠ¸ ë³´ê¸°"):
        #    st.code(prompt, language="markdown")

        add_message("assistant", "ì´ì œ AI ìƒë‹´ì‚¬ê°€ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ìƒì„±í•©ë‹ˆë‹¤... â³")

        with st.chat_message("assistant"):
            st.markdown("### ğŸ“ˆ ìƒì„±ëœ ë§ˆì¼€íŒ… ì „ëµ ê²°ê³¼")
            content_placeholder = st.empty()
            result = stream_gemini(prompt, output_placeholder=content_placeholder)  # â¬…ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
            if result:
                summary_points = extract_executive_summary(result)
                if summary_points:
                    summary_markdown = "#### âš¡ í•µì‹¬ ìš”ì•½\n\n" + "\n".join(
                        f"- {point}" for point in summary_points
                    )
                    combined_result = f"{summary_markdown}\n\n---\n\n{result}"
                    content_placeholder.markdown(combined_result)
                    st.session_state.chat_history.append(
                        {"role": "assistant", "content": combined_result}
                    )
                else:
                    st.session_state.chat_history.append({"role": "assistant", "content": result})
