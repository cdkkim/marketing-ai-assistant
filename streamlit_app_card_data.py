# -*- coding: utf-8 -*-
import os
import io
import csv
import re
import json
import logging
import time
import uuid
import streamlit as st
import google.generativeai as genai

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. ë¡œê·¸ ë° ê²½ê³  ì–µì œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.getLogger("google.auth").setLevel(logging.ERROR)
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "3"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Gemini API ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    st.warning("âš ï¸ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Streamlit secretsì— ì¶”ê°€í•˜ì„¸ìš”.")

DEFAULT_MODEL = "gemini-2.5-flash"

DATA_EVIDENCE_GUIDE = (
    "\n\nì¶”ê°€ ì§€ì¹¨:\n"
    "- ê° ì œì•ˆì—ëŠ” ë°ì´í„° ê·¼ê±°(í‘œ/ì§€í‘œ/ê·œì¹™ ë“±)ë¥¼ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”.\n"
    "- ê°€ëŠ¥í•œ ê²½ìš° ê°„ë‹¨í•œ í‘œë‚˜ ì§€í‘œ ìˆ˜ì¹˜ë¥¼ í™œìš©í•´ ê·¼ê±°ë¥¼ ëª…í™•íˆ ë³´ì—¬ì£¼ì„¸ìš”."
)

STRUCTURED_RESPONSE_GUIDE = (
    "\n\nì‘ë‹µ í˜•ì‹ ì§€ì¹¨(ì¤‘ìš”):\n"
    "1. ë°˜ë“œì‹œ ë°±í‹±ì´ë‚˜ ì£¼ì„ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
    "2. JSONì€ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”.\n"
    "{\n"
    '  \"objective\": \"ìµœìš°ì„  ë§ˆì¼€íŒ… ëª©í‘œë¥¼ 1ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½\",\n'
    '  \"phase_titles\": [\"Phase 1: â€¦\", \"Phase 2: â€¦\", \"Phase 3: â€¦\"],\n'
    '  \"channel_summary\": [\n'
    '    {\n'
    '      \"channel\": \"ì±„ë„ëª…\",\n'
    '      \"phase_title\": \"ì—°ê²°ëœ Phase ì œëª©\",\n'
    '      \"reason\": \"ì¶”ì²œ ì´ìœ ì™€ ê¸°ëŒ€ íš¨ê³¼\",\n'
    '      \"data_evidence\": \"ê´€ë ¨ ìˆ˜ì¹˜/ê·œì¹™ ë“± ë°ì´í„° ê·¼ê±°\"\n'
    "    }\n"
    "  ],\n"
    '  \"phases\": [\n'
    "    {\n"
    '      \"title\": \"Phase 1: â€¦\",\n'
    '      \"goal\": \"êµ¬ì²´ì ì¸ ëª©í‘œ\",\n'
    '      \"focus_channels\": [\"í•µì‹¬ ì±„ë„ 1\", \"í•µì‹¬ ì±„ë„ 2\"],\n'
    '      \"actions\": [\n'
    "        {\n"
    '          \"task\": \"ì²´í¬ë°•ìŠ¤ì— ë“¤ì–´ê°ˆ ì‹¤í–‰ í•­ëª©\",\n'
    '          \"owner\": \"ë‹´ë‹¹ ì—­í• (ì˜ˆ: ì ì£¼, ìŠ¤íƒœí”„)\",\n'
    '          \"supporting_data\": \"ì„ íƒ) ê´€ë ¨ ë°ì´í„° ê·¼ê±°\"\n'
    "        }\n"
    "      ],\n"
    '      \"metrics\": [\"ì„±ê³¼ KPI\"],\n'
    '      \"next_phase_criteria\": [\"ë‹¤ìŒ Phaseë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ ì •ëŸ‰/ì •ì„± ê¸°ì¤€\"],\n'
    '      \"data_evidence\": [\"Phase ì „ëµì„ ë’·ë°›ì¹¨í•˜ëŠ” ê·¼ê±°\"]\n'
    "    }\n"
    "  ],\n"
    '  \"risks\": [\"ì£¼ìš” ë¦¬ìŠ¤í¬ì™€ ëŒ€ì‘ ìš”ì•½\"],\n'
    '  \"monitoring_cadence\": \"ëª¨ë‹ˆí„°ë§ ì£¼ê¸°ì™€ ì±…ì„ì\"\n'
    "}\n"
    "3. PhaseëŠ” ì‹œê°„ ìˆœì„œë¥¼ ì§€í‚¤ê³  Phase 1ì˜ action í•­ëª©ì€ ìµœì†Œ 3ê°œë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
    "4. ëª¨ë“  reason, supporting_data, data_evidenceì—ëŠ” ì •ëŸ‰ ìˆ˜ì¹˜ë‚˜ ê·œì¹™ì  ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”."
)


FOLLOWUP_RESPONSE_GUIDE = (
    "\n\nì‘ë‹µ í˜•ì‹ ì§€ì¹¨(ì¤‘ìš”):\n"
    "1. ë°˜ë“œì‹œ ë°±í‹±ì´ë‚˜ ì£¼ì„ ì—†ì´ ìˆœìˆ˜ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
    "2. JSONì€ ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ì„¸ìš”.\n"
    "{\n"
    '  "summary_points": ["í•µì‹¬ ìš”ì•½ 1", "í•µì‹¬ ìš”ì•½ 2"],\n'
    '  "detailed_guidance": "ìš”ì•½ì„ í™•ì¥í•˜ëŠ” ìƒì„¸ ì¡°ì–¸",\n'
    '  "evidence_mentions": ["ê´€ë ¨ ê·¼ê±° ë˜ëŠ” KPI ì–¸ê¸‰"],\n'
    '  "suggested_question": "ë‹¤ìŒìœ¼ë¡œ ì´ì–´ì§ˆ ê°„ë‹¨í•œ í•œ ë¬¸ì¥ ì§ˆë¬¸"\n'
    "}\n"
    "3. summary_pointsëŠ” ìµœëŒ€ 2ê°œ, ê° í•­ëª©ì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³  ì¤‘í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
    "4. evidence_mentionsëŠ” ìµœëŒ€ 3ê°œ ì´ë‚´ì˜ ë¶ˆë¦¿ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ìˆ«ìë‚˜ ì§€í‘œê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.\n"
    "5. detailed_guidanceëŠ” ê¸°ì¡´ ì „ëµì„ ì¬í•´ì„í•˜ë©° ë°ì´í„° ê·¼ê±°ë¥¼ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ë˜ ì‰¬ìš´ ì–´íœ˜ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "6. suggested_questionì€ ì‚¬ìš©ìê°€ ë°”ë¡œ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§§ì€ í›„ì† ì§ˆë¬¸ 1ê°œë§Œ ì œì•ˆí•˜ì„¸ìš”."
)

def ensure_data_evidence(prompt: str) -> str:
    """í”„ë¡¬í”„íŠ¸ì— ë°ì´í„° ê·¼ê±° ì§€ì¹¨ì´ ì—†ìœ¼ë©´ ì¶”ê°€."""
    updated = prompt.rstrip()
    if "ë°ì´í„° ê·¼ê±°" not in updated:
        updated += DATA_EVIDENCE_GUIDE
    if '"phase_titles"' not in updated and "ì‘ë‹µ í˜•ì‹ ì§€ì¹¨(ì¤‘ìš”)" not in updated:
        updated += STRUCTURED_RESPONSE_GUIDE
    return updated

def extract_executive_summary(markdown_text: str, max_points: int = 4):
    """ìƒì„±ëœ ì „ëµ ë³¸ë¬¸ì—ì„œ ìš”ì•½ ì„¹ì…˜ì˜ í•µì‹¬ ë¶ˆë¦¿ì„ ì¶”ì¶œ."""
    lines = markdown_text.splitlines()
    summary_lines = []

    def clean_bullet(line):
        stripped = line.strip()
        if not stripped:
            return None
        bullet_match = re.match(r"^[-\*\u2022]\s*(.+)", stripped)
        if bullet_match:
            return bullet_match.group(1).strip()
        numbered_match = re.match(r"^\d+[.)]\s*(.+)", stripped)
        if numbered_match:
            return numbered_match.group(1).strip()
        return None

    heading_pattern = re.compile(r"#{1,6}\s*ìš”ì•½")
    start_idx = next(
        (idx for idx, line in enumerate(lines) if heading_pattern.match(line.strip())),
        None,
    )

    if start_idx is not None:
        for line in lines[start_idx + 1 :]:
            stripped = line.strip()
            if stripped.startswith("#"):
                break
            cleaned = clean_bullet(stripped)
            if cleaned:
                summary_lines.append(cleaned)
            if len(summary_lines) >= max_points:
                break

    if not summary_lines:
        for line in lines:
            cleaned = clean_bullet(line)
            if cleaned:
                summary_lines.append(cleaned)
            if len(summary_lines) >= max_points:
                break

    if not summary_lines:
        collapsed = re.sub(r"\s+", " ", markdown_text)
        sentences = re.split(r"(?<=[.!?])\s", collapsed)
        for sentence in sentences:
            cleaned = sentence.strip()
            if cleaned:
                summary_lines.append(cleaned)
            if len(summary_lines) >= max_points:
                break

    return summary_lines

def strip_json_artifacts(text: str) -> str:
    """ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì„ì¼ ìˆ˜ ìˆëŠ” ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•˜ê³  ìˆœìˆ˜ JSON ë¬¸ìì—´ë§Œ ë‚¨ê¸´ë‹¤."""
    cleaned = text.strip().replace("â–Œ", "")
    fence_pattern = re.compile(r"^```(?:json)?\s*|\s*```$")
    cleaned = fence_pattern.sub("", cleaned)
    return cleaned.strip()

def parse_strategy_payload(raw_text: str):
    """JSON ì‘ë‹µì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±. ì‹¤íŒ¨ ì‹œ None."""
    candidate = strip_json_artifacts(raw_text)
    if not candidate:
        return None
    try:
        return json.loads(candidate)
    except json.JSONDecodeError:
        return None

def parse_followup_payload(raw_text: str):
    """í›„ì† ì§ˆì˜ ì‘ë‹µìš© JSONì„ íŒŒì‹±."""
    candidate = strip_json_artifacts(raw_text)
    if not candidate:
        return None
    try:
        data = json.loads(candidate)
    except json.JSONDecodeError:
        return None
    if not isinstance(data, dict):
        return None
    return data

INFO_FIELD_ORDER = ["ìƒì ëª…", "ì í¬ì—°ë ¹", "ê³ ê°ì—°ë ¹ëŒ€", "ê³ ê°í–‰ë™"]
BUTTON_HINT = "\n\ní•„ìš”í•œ ì •ë³´ê°€ ì•„ë‹ˆì–´ë„ ì•„ë˜ 'ì´ëŒ€ë¡œ ì§ˆë¬¸' ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì§€ê¸ˆ ì •ë³´ë¡œ ë°”ë¡œ ë‹µë³€ì„ ë“œë¦´ê²Œìš”."
DIRECT_RESPONSE_GUIDE = (
    "\n\në‹µë³€ ì§€ì¹¨:\n"
    "- ë¶ˆë¦¿ ëŒ€ì‹  1~2ê°œì˜ ì§§ì€ ë‹¨ë½ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "- ì¤‘í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n"
    "- ê°€ëŠ¥í•œ ê²½ìš° ìˆ«ìë‚˜ ê·œì¹™ ê°™ì€ ê·¼ê±°ë¥¼ ë¬¸ì¥ ì•ˆì— ì§ì ‘ ë…¹ì—¬ ì£¼ì„¸ìš”.\n"
    "- ì‹¤í–‰ ì•„ì´ë””ì–´ëŠ” êµ¬ì²´ì ì¸ ì˜ˆì‹œì™€ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.\n"
    "- ë§ˆì§€ë§‰ì—ëŠ” `ì¶”ì²œ í›„ì† ì§ˆë¬¸: â€¦` í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì´ì–´ì„œ ë¬¼ì–´ë³¼ ë§Œí•œ ì§ˆë¬¸ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”."
)

def get_missing_info_fields(info: dict) -> list:
    """í•„ìˆ˜ ì •ë³´ ì¤‘ ì•„ì§ ìˆ˜ì§‘ë˜ì§€ ì•Šì€ í•­ëª©ì„ ë°˜í™˜."""
    missing = []
    for field in INFO_FIELD_ORDER:
        value = info.get(field)
        if not value:
            missing.append(field)
    return missing

def get_latest_strategy_message():
    """ì„¸ì…˜ ê¸°ë¡ì—ì„œ ê°€ì¥ ìµœì‹  ì „ëµ ë©”ì‹œì§€ë¥¼ ë°˜í™˜."""
    history = st.session_state.get("chat_history", [])
    for item in reversed(history):
        if item.get("type") == "strategy":
            return item
    return None

def build_followup_prompt(question: str, info: dict, strategy_payload: dict, raw_strategy: str) -> str:
    """ì´ì „ ì „ëµì„ ë¬¸ë§¥ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    info_keys = ["ìƒì ëª…", "ì—…ì¢…", "í”„ëœì°¨ì´ì¦ˆì—¬ë¶€", "ì í¬ì—°ë ¹", "ê³ ê°ì—°ë ¹ëŒ€", "ê³ ê°í–‰ë™"]
    info_lines = [
        f"- {key}: {info[key]}"
        for key in info_keys
        if key in info and info[key]
    ]
    info_block = "\n".join(info_lines) if info_lines else "- ì¶”ê°€ ìƒì  ì •ë³´ ì—†ìŒ"

    strategy_block = ""
    if strategy_payload:
        try:
            strategy_block = json.dumps(strategy_payload, ensure_ascii=False, indent=2)
        except (TypeError, ValueError):
            strategy_block = raw_strategy or ""
    else:
        strategy_block = raw_strategy or ""

    prompt = (
        "ë‹¹ì‹ ì€ ì¤‘ì†Œìƒê³µì¸ì„ ë•ëŠ” ì‹œë‹ˆì–´ ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì´ë¯¸ ìƒì„±ëœ ì „ëµ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í›„ì† ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì „ëµì˜ Phase, ì±„ë„, ì‹¤í–‰ í•­ëª©, ë°ì´í„° ê·¼ê±°ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì¸ìš©í•˜ê³  í•„ìš” ì‹œ ê°„ë‹¨í•œ ì¶”ê°€ ì¡°ì–¸ì„ ë”í•˜ì„¸ìš”.\n"
        "ìƒˆ ì „ëµì„ ìƒˆë¡œ ì§œì§€ ë§ê³ , ê¸°ì¡´ ì „ëµì„ ì¬í•´ì„í•˜ê±°ë‚˜ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ëª¨ë“  ì„¤ëª…ì€ ì¤‘í•™ìƒì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
        "=== ìƒì  ê¸°ë³¸ ì •ë³´ ===\n"
        f"{info_block}\n\n"
        "=== ê¸°ì¡´ ì „ëµ(JSON) ===\n"
        f"{strategy_block}\n\n"
        "=== ì‚¬ìš©ì ì§ˆë¬¸ ===\n"
        f"{question}\n\n"
        "ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì „ëµ ì •ë³´ë¥¼ ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê·¼ê±°ë¡œ í™œìš©í•´ ì¡°ì–¸í•˜ì„¸ìš”.\n"
        "ë°ì´í„° ê·¼ê±° í•­ëª©ì´ë‚˜ KPIê°€ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì–¸ê¸‰í•˜ê±°ë‚˜ ìˆ˜ì¹˜ë¡œ ë‹µë³€ì— ë°˜ì˜í•˜ì„¸ìš”."
        f"{FOLLOWUP_RESPONSE_GUIDE}"
    )
    return prompt

def build_direct_question_prompt(info: dict, question: str, missing_fields=None) -> str:
    """ìˆ˜ì§‘ëœ ì •ë³´ë§Œìœ¼ë¡œ ì§ì ‘ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±."""
    missing_fields = missing_fields or []
    info_lines = [
        f"- {field}: {info[field]}"
        for field in INFO_FIELD_ORDER
        if info.get(field)
    ]
    info_block = "\n".join(info_lines) if info_lines else "- ì œê³µëœ ì •ë³´ ì—†ìŒ"

    missing_note = ""
    if missing_fields:
        missing_note = (
            "\n\nì£¼ì˜: ì•„ì§ ë‹¤ìŒ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            + ", ".join(missing_fields)
            + "."
        )

    prompt = (
        "ë‹¹ì‹ ì€ ë™ë„¤ ìƒê¶Œì„ ë•ëŠ” ì‹œë‹ˆì–´ ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì•„ë˜ ìƒì  ì •ë³´ë¥¼ ì°¸ê³ í•´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë°”ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ì¡°ì–¸ì„ ì£¼ì„¸ìš”.\n"
        "ë‹µë³€ì€ ë¬¸ë‹¨ í˜•íƒœë¡œ ì‘ì„±í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìˆ˜ì¹˜ë‚˜ ê·œì¹™ ê°™ì€ ê·¼ê±°ë¥¼ ë¬¸ì¥ ì•ˆì— ë…¹ì—¬ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ìƒˆë¡œìš´ ê°€ì •ì„ ë§Œë“¤ê¸°ë³´ë‹¤ëŠ” ì œê³µëœ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.\n\n"
        "=== ìƒì  ì •ë³´ ===\n"
        f"{info_block}\n\n"
        "=== ì‚¬ìš©ì ì§ˆë¬¸ ===\n"
        f"{question}\n"
        f"{missing_note}"
        f"{DIRECT_RESPONSE_GUIDE}"
    )
    return prompt

def default_suggested_question(info: dict, question: str) -> str:
    """ì§ˆë¬¸ì´ë‚˜ ìƒì  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ë³¸ í›„ì† ì§ˆë¬¸ì„ ë„ì¶œ."""
    text = (question or "").lower()
    if "ë‹¨ê³¨" in text or "ì¬ë°©ë¬¸" in text:
        return "ë‹¨ê³¨ ê³ ê°ì—ê²Œ ì¤„ë§Œí•œ í˜œíƒ ì•„ì´ë””ì–´ë„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"
    if "ë§¤ì¶œ" in text or "íŒë§¤" in text or "ì‹¤ì " in text:
        return "ë§¤ì¶œì„ ë” ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ì¶”ê°€ í”„ë¡œëª¨ì…˜ì´ ìˆì„ê¹Œìš”?"
    if "ì‹ ê·œ" in text or "ìƒˆ" in text:
        return "ì‹ ê·œ ì†ë‹˜ì„ ëŠ˜ë¦¬ë ¤ë©´ ì–´ë–¤ í™ë³´ ì±„ë„ì´ ì¢‹ì„ê¹Œìš”?"
    if "ê´‘ê³ " in text or "í™ë³´" in text or "ë§ˆì¼€íŒ…" in text:
        return "ê´‘ê³  ì˜ˆì‚°ì€ ì–´ëŠ ì •ë„ë¡œ ì¡ìœ¼ë©´ ì¢‹ì„ê¹Œìš”?"
    age = info.get("ê³ ê°ì—°ë ¹ëŒ€", "")
    if "30" in age or "40" in age:
        return "30~40ëŒ€ì—ê²Œ ë°˜ì‘ ì¢‹ì€ ì½˜í…ì¸  ì˜ˆì‹œë¥¼ ë” ì•Œë ¤ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"
    if "50" in age or "60" in age:
        return "50ëŒ€ ê³ ê°ì´ ì¢‹ì•„í•  ì´ë²¤íŠ¸ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì¶”ì²œí•´ ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"
    return "SNS í™ë³´ ì „ëµë„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì„ê¹Œìš”?"

def parse_direct_answer(answer_text: str, info: dict, question: str) -> tuple:
    """ì§ì ‘ ë‹µë³€ì—ì„œ ìƒì„¸ ê°€ì´ë“œì™€ ì¶”ì²œ ì§ˆë¬¸ì„ ë¶„ë¦¬."""
    if not answer_text:
        return "", ""

    guidance = answer_text.strip()
    suggested_question = ""
    match = re.search(r"ì¶”ì²œ\s*í›„ì†\s*ì§ˆë¬¸\s*:\s*(.+)$", guidance, flags=re.IGNORECASE | re.MULTILINE)
    if match:
        suggested_question = match.group(1).strip()
        guidance = guidance[: match.start()].strip()

    if not suggested_question:
        suggested_question = default_suggested_question(info, question)

    return guidance, suggested_question

def render_strategy_payload(payload: dict, container, prefix: str = "latest"):
    """êµ¬ì¡°í™”ëœ ì „ëµ ì‘ë‹µì„ Streamlit ì»´í¬ë„ŒíŠ¸ë¡œ ì‹œê°í™”."""
    objective = payload.get("objective")
    if objective:
        container.markdown("### ğŸ¯ Objective")
        container.markdown(objective)

    phase_titles = payload.get("phase_titles") or []
    channel_summary = payload.get("channel_summary") or []
    if channel_summary:
        container.markdown("### ğŸ“Š Recommended Channels & Phase Titles")
        summary_lines = []
        for item in channel_summary:
            channel = item.get("channel", "ì±„ë„ ë¯¸ì§€ì •")
            phase_title = item.get("phase_title", "Phase ë¯¸ì§€ì •")
            reason = item.get("reason", "")
            evidence = item.get("data_evidence", "")
            detail = f"- **{channel}** â†’ {phase_title}: {reason}"
            if evidence:
                detail += f" _(ê·¼ê±°: {evidence})_"
            summary_lines.append(detail)
        container.markdown("\n".join(summary_lines))
        if phase_titles:
            container.markdown("**Phase Titles:** " + ", ".join(phase_titles))
    elif phase_titles:
        container.markdown("### ğŸ“‹ Phase Titles")
        container.markdown(", ".join(phase_titles))

    phases = payload.get("phases") or []
    if not phases:
        return

    # Phase 1 ìš°ì„  í‘œì‹œ
    phase1 = phases[0]
    phase1_container = container.container()
    phase1_container.markdown(f"### ğŸš€ {phase1.get('title', 'Phase 1')}")
    goal = phase1.get("goal")
    if goal:
        phase1_container.markdown(f"**Goal:** {goal}")
    focus_channels = phase1.get("focus_channels") or []
    if focus_channels:
        phase1_container.markdown("**Focus Channels:** " + ", ".join(focus_channels))

    actions = phase1.get("actions") or []
    if actions:
        phase1_container.markdown("**Action Checklist:**")
        for idx, action in enumerate(actions):
            label = action.get("task", f"Action {idx + 1}")
            owner = action.get("owner")
            support = action.get("supporting_data")
            help_parts = []
            if owner:
                help_parts.append(f"ë‹´ë‹¹: {owner}")
            if support:
                help_parts.append(f"ê·¼ê±°: {support}")
            help_text = " | ".join(help_parts) if help_parts else None
            checkbox_key = f"{prefix}_phase1_action_{idx}"
            phase1_container.checkbox(label, key=checkbox_key, help=help_text)

    metrics = phase1.get("metrics") or []
    if metrics:
        phase1_container.markdown("**Metrics:**")
        phase1_container.markdown("\n".join(f"- {m}" for m in metrics))

    criteria = phase1.get("next_phase_criteria") or []
    if criteria:
        phase1_container.markdown("**Criteria To Advance:**")
        phase1_container.markdown("\n".join(f"- {c}" for c in criteria))

    evidence = phase1.get("data_evidence") or []
    if evidence:
        phase1_container.markdown("**Data Evidence:**")
        phase1_container.markdown("\n".join(f"- {e}" for e in evidence))

    # ë‚˜ë¨¸ì§€ PhaseëŠ” Expanderë¡œ í‘œì‹œ
    for idx, phase in enumerate(phases[1:], start=2):
        title = phase.get("title", f"Phase {idx}")
        expander = container.expander(title, expanded=False)
        goal = phase.get("goal")
        if goal:
            expander.markdown(f"**Goal:** {goal}")
        focus_channels = phase.get("focus_channels") or []
        if focus_channels:
            expander.markdown("**Focus Channels:** " + ", ".join(focus_channels))

        actions = phase.get("actions") or []
        if actions:
            expander.markdown("**Key Actions:**")
            expander.markdown("\n".join(f"- [ ] {act.get('task', 'ì‘ì—… ë¯¸ì •')}" for act in actions))

        metrics = phase.get("metrics") or []
        if metrics:
            expander.markdown("**Metrics:**")
            expander.markdown("\n".join(f"- {m}" for m in metrics))

        criteria = phase.get("next_phase_criteria") or []
        if criteria:
            expander.markdown("**Criteria To Advance:**")
            expander.markdown("\n".join(f"- {c}" for c in criteria))

        evidence = phase.get("data_evidence") or []
        if evidence:
            expander.markdown("**Data Evidence:**")
            expander.markdown("\n".join(f"- {e}" for e in evidence))

    risks = payload.get("risks") or []
    monitoring_cadence = payload.get("monitoring_cadence")
    if risks or monitoring_cadence:
        container.markdown("### âš ï¸ Risks & Monitoring")
        if risks:
            container.markdown("\n".join(f"- {r}" for r in risks))
        if monitoring_cadence:
            container.markdown(f"**Monitoring Cadence:** {monitoring_cadence}")

def render_followup_panel(guidance_text: str, evidence_list, suggested_question: str, ui_key: int):
    """Follow-up ì‘ë‹µì„ ìƒì„¸ ê°€ì´ë“œì™€ í›„ì† ì§ˆë¬¸ ë²„íŠ¼ìœ¼ë¡œ í‘œì‹œ."""
    if not guidance_text:
        return

    st.markdown("### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ")
    st.markdown(guidance_text)

    evidence_items = evidence_list or []
    if evidence_items:
        st.markdown("**ê·¼ê±°:**")
        st.markdown("\n".join(f"- {item}" for item in evidence_items))

    first_time = not st.session_state.get("shown_followup_suggestion", False)
    if first_time:
        st.session_state.shown_followup_suggestion = True
        if suggested_question:
            st.markdown(f"**ê°€ëŠ¥í•œ ë‹¤ìŒ ì§ˆë¬¸:** {suggested_question}")

    col1, col2 = st.columns(2)
    suggestion_clicked = False
    if suggested_question:
        suggestion_clicked = col1.button(
            suggested_question,
            key=f"followup_suggest_{ui_key}",
            use_container_width=True,
            disabled=st.session_state.get("is_generating", False),
        )
    else:
        col1.write("")
    other_clicked = col2.button(
        "ë‹¤ë¥¸ ì§ˆë¬¸ ì…ë ¥",
        key=f"followup_new_{ui_key}",
        use_container_width=True,
        disabled=st.session_state.get("is_generating", False),
    )

    if suggestion_clicked:
        st.session_state.followup_ui = {}
        st.session_state.auto_followup_question = suggested_question
        st.rerun()

    if other_clicked:
        st.session_state.followup_ui = {}
        st.rerun()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Persona ë°ì´í„° ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_personas(path="personas.json"):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        st.error("âš ï¸ personas.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. persona_generator.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return []

personas = load_personas()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì—…ì¢… ë¶„ë¥˜ & í”„ëœì°¨ì´ì¦ˆ íŒë³„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BRAND_KEYWORDS_BY_CATEGORY = {
    "ì¹´í˜/ë””ì €íŠ¸": {
        "íŒŒë¦¬","ëšœë ˆ","ë°°ìŠ¤","ë² ìŠ¤","ë² ìŠ¤í‚¨","ë˜í‚¨","í¬ë¦¬ìŠ¤í”¼","íˆ¬ì¸","ì´ë””","ë¹½ë‹¤","ë©”ê°€",
        "ë©”ë¨¸","ë§¤ë¨¸","ì»´í¬","ì»´í¬ì¦ˆ","í• ë¦¬","ìŠ¤íƒ€ë²…","íƒì•¤","ê³µì°¨","ìš”ê±°","ì™€í”ŒëŒ€","ì™€í”Œ","í´ë°”ì…‹",
    },
    "í•œì‹": {
        "êµì´Œ","ë„¤ë„¤","í˜¸ì‹","ë‘˜ë‘˜","ì²˜ê°“","êµ½ë„¤","bbq","bhc","ë§˜ìŠ¤","ë§˜ìŠ¤í„°ì¹˜","ì£ ìŠ¤","ì‹ ì „","êµ­ëŒ€",
        "ëª…ë‘","ë‘ë¼","ë•…ìŠ¤","ëª…ë¥œ","í•˜ë‚¨","ë“±ì´Œ","ë´‰ì¶”","ì›í• ","ë³¸ì£½","ì›í• ë¨¸ë‹ˆ","í•œì´Œ","ë°±ì±„","í•œì†¥","ë°”ë¥´",
    },
    "ì–‘ì‹/ì„¸ê³„ìš”ë¦¬": {"ë„ë¯¸","í”¼ìí—›","íŒŒíŒŒ","íŒŒíŒŒì¡´ìŠ¤","ë¡¯ë°","ë²„ê±°í‚¹","ì¨ë¸Œ","ì„œë¸Œì›¨ì´","ì´ì‚­","í”„ë­","í”„ë­í¬","í”¼ììŠ¤ì¿¨"},
    "ì£¼ì /ì£¼ë¥˜": {"í•œì‹ "},
}

def _normalize_name(name: str) -> str:
    n = name.lower()
    n = re.sub(r"[\s\*\-\(\)\[\]{}_/\\.|,!?&^%$#@~`+=:;\"']", "", n)
    return n

def classify_hpsn_mct(name: str) -> str:
    normalized = _normalize_name(name)
    for category, keywords in BRAND_KEYWORDS_BY_CATEGORY.items():
        if any(k in normalized for k in keywords):
            return category

    nm = name.strip().lower()
    if any(k in nm for k in ["ì¹´í˜","ì»¤í”¼","ë””ì €íŠ¸","ë„ë„ˆì¸ ","ë¹™ìˆ˜","ì™€í”Œ","ë§ˆì¹´ë¡±"]): return "ì¹´í˜/ë””ì €íŠ¸"
    if any(k in nm for k in ["í•œì‹","êµ­ë°¥","ë°±ë°˜","ì°Œê°œ","ê°ìíƒ•","ë¶„ì‹","ì¹˜í‚¨","í•œì •ì‹","ì£½"]): return "í•œì‹"
    if any(k in nm for k in ["ì¼ì‹","ì´ˆë°¥","ëˆê°€ìŠ¤","ë¼ë©˜","ë®ë°¥","ì†Œë°”","ì´ìì¹´ì•¼"]): return "ì¼ì‹"
    if any(k in nm for k in ["ì¤‘ì‹","ì§¬ë½•","ì§œì¥","ë§ˆë¼","í› ê¶ˆ","ë”¤ì„¬"]): return "ì¤‘ì‹"
    if any(k in nm for k in ["ì–‘ì‹","ìŠ¤í…Œì´í¬","í”¼ì","íŒŒìŠ¤íƒ€","í–„ë²„ê±°","ìƒŒë“œìœ„ì¹˜","í† ìŠ¤íŠ¸","ë²„ê±°"]): return "ì–‘ì‹/ì„¸ê³„ìš”ë¦¬"
    if any(k in nm for k in ["ì£¼ì ","í˜¸í”„","ë§¥ì£¼","ì™€ì¸ë°”","ì†Œì£¼","ìš”ë¦¬ì£¼ì ","ì´ìì¹´ì•¼"]): return "ì£¼ì /ì£¼ë¥˜"
    return "ê¸°íƒ€"

BRAND_KEYWORDS = {kw for kws in BRAND_KEYWORDS_BY_CATEGORY.values() for kw in kws}
AMBIGUOUS_NEGATIVES = {"ì¹´í˜","ì»¤í”¼","ì™•ì‹­","ì„±ìˆ˜","í–‰ë‹¹","ì¢…ë¡œ","ì „ì£¼","ì¶˜ì²œ","ì™€ì¸","ì¹˜í‚¨","í”¼ì","ë¶„ì‹","êµ­ìˆ˜","ì´ˆë°¥","ìŠ¤ì‹œ","ê³±ì°½","ë¼ì§€","í•œìš°","ë§‰ì°½","ìˆ˜ì‚°","ì¶•ì‚°","ë² ì´","ë¸Œë ˆ","ë¸Œë ˆë“œ"}

def is_franchise(name: str) -> bool:
    n = _normalize_name(name)
    if not n:
        return False
    has_branch_marker = "ì " in name
    hit = any(k in n for k in BRAND_KEYWORDS)
    if hit:
        if any(bad in n for bad in AMBIGUOUS_NEGATIVES):
            short_hits = [k for k in BRAND_KEYWORDS if k in n and len(k) <= 2]
            if short_hits and not has_branch_marker:
                return False
        return True
    return False

def _store_age_label_from_months(months: int) -> str:
    if months <= 12: return "ì‹ ê·œ"
    if months <= 24: return "ì „í™˜ê¸°"
    return "ì˜¤ë˜ëœ"

def extract_initial_store_info(text: str) -> tuple:
    """ë³µí•© ë¬¸ì¥ì—ì„œ ìƒì  ì •ë³´ì™€ ì§ˆë¬¸ì„ ë¶„ë¦¬í•´ ì¶”ì¶œ."""
    info_updates = {}
    question = None
    if not text:
        return info_updates, question

    sentences = re.split(r"(?<=[?.!])\s+", text.strip())
    info_sentences, question_sentences = [], []
    for sentence in sentences:
        stripped = sentence.strip()
        if not stripped:
            continue
        if "?" in stripped or stripped.endswith("ê¹Œìš”") or stripped.endswith("í•  ìˆ˜ ìˆì„ê¹Œìš”") or "ì–´ë–»ê²Œ" in stripped:
            question_sentences.append(stripped)
        else:
            info_sentences.append(stripped)

    if not question_sentences and info_sentences:
        question_sentences.append(info_sentences.pop())

    context_text = " ".join(info_sentences) if info_sentences else text.strip()
    question = " ".join(question_sentences).strip() if question_sentences else text.strip()

    normalized_no_space = _normalize_name(context_text)
    brand_hits = [kw for kw in BRAND_KEYWORDS if kw in normalized_no_space]
    if brand_hits:
        store_name = max(brand_hits, key=len)
        info_updates["ìƒì ëª…"] = store_name
    else:
        name_match = re.search(r"([ê°€-í£A-Za-z0-9]+)(?:ì )?(?:ì…ë‹ˆë‹¤|ì´ì—ìš”|ì˜ˆìš”|ì—ìš”)", context_text)
        if name_match:
            info_updates["ìƒì ëª…"] = name_match.group(1)

    age_months = None
    year_match = re.search(r"(\d+)\s*(?:ë…„|ë…„ì°¨|ë…„ì§¸)", text)
    month_match = re.search(r"(\d+)\s*(?:ê°œì›”|ë‹¬)", text)
    if year_match:
        age_months = int(year_match.group(1)) * 12
    elif month_match:
        age_months = int(month_match.group(1))

    if age_months is not None:
        info_updates["ì í¬ì—°ë ¹"] = _store_age_label_from_months(age_months)

    if re.search(r"20\s*ëŒ€", text):
        info_updates["ê³ ê°ì—°ë ¹ëŒ€"] = "20ëŒ€ ì´í•˜ ê³ ê° ì¤‘ì‹¬"
    elif re.search(r"(?:30|40)\s*ëŒ€", text):
        info_updates["ê³ ê°ì—°ë ¹ëŒ€"] = "30~40ëŒ€ ê³ ê° ì¤‘ì‹¬"
    elif re.search(r"(?:50|60)\s*ëŒ€", text):
        info_updates["ê³ ê°ì—°ë ¹ëŒ€"] = "50ëŒ€ ì´ìƒ ê³ ê° ì¤‘ì‹¬"

    behaviors = []
    if "ë‹¨ê³¨" in text or "ì¬ë°©ë¬¸" in text: behaviors.append("ì¬ë°©ë¬¸ ê³ ê°")
    if "ì‹ ê·œ" in text or "ìƒˆì†ë‹˜" in text or "ìƒˆ ì†ë‹˜" in text: behaviors.append("ì‹ ê·œ ê³ ê°")
    if "ê±°ì£¼" in text or "ì£¼ë¯¼" in text: behaviors.append("ê±°ì£¼ ê³ ê°")
    if "ì§ì¥" in text or "ì˜¤í”¼ìŠ¤" in text or "íšŒì‚¬" in text: behaviors.append("ì§ì¥ì¸ ê³ ê°")
    if "ìœ ë™" in text or "ì§€ë‚˜ê°€ëŠ”" in text or "ê´€ê´‘" in text: behaviors.append("ìœ ë™ ê³ ê°")
    if behaviors:
        info_updates["ê³ ê°í–‰ë™"] = " + ".join(sorted(set(behaviors)))

    return info_updates, question

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Gemini Streaming í˜¸ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_MODEL = "gemini-2.5-flash"

def stream_gemini(
    prompt,
    model=DEFAULT_MODEL,
    temperature=0.6,
    max_tokens=65535,
    output_placeholder=None,
    status_text="ì „ëµì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤... â³",
    progress_text="AIê°€ ì „ëµì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ“‹",
    success_text="âœ… ì „ëµ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
    error_status_text="ğŸš¨ ì „ëµ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
):
    """ì•ˆì •ì ì¸ ìŠ¤íŠ¸ë¦¬ë° + ì™„ë£Œì‚¬ìœ  ì ê²€ + ì¹œì ˆí•œ ì—ëŸ¬"""
    status_placeholder = st.empty()
    status_placeholder.info(status_text)

    try:
        gmodel = genai.GenerativeModel(model)
        cfg = {"temperature": temperature, "max_output_tokens": max_tokens, "top_p": 0.9, "top_k": 40}
        stream = gmodel.generate_content(prompt, generation_config=cfg, stream=True)

        placeholder = output_placeholder or st.empty()
        placeholder.info(progress_text)
        full_text = ""

        for event in stream:
            piece = ""
            if getattr(event, "text", None):
                piece = event.text
            elif getattr(event, "candidates", None):
                for c in event.candidates:
                    try:
                        piece += "".join([p.text or "" for p in c.content.parts])
                    except Exception:
                        pass
            if piece:
                full_text += piece

        try:
            stream.resolve()
        except Exception:
            pass

        if not full_text:
            placeholder.warning("ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")

        try:
            cand0 = stream.candidates[0]
            fr = getattr(cand0, "finish_reason", None)
        except Exception:
            fr = None

        if fr == "MAX_TOKENS":
            st.info("â„¹ï¸ ì‘ë‹µì´ ê¸¸ì–´ ì¤‘ê°„ì— ì˜ë ¸ì–´ìš”. ì•„ë˜ ë²„íŠ¼ìœ¼ë¡œ ì´ì–´ì„œ ìƒì„±í•  ìˆ˜ ìˆì–´ìš”.")
        elif fr == "SAFETY":
            st.warning("âš ï¸ ì•ˆì „ í•„í„°ë¡œ ì¼ë¶€ ë‚´ìš©ì´ ìˆ¨ê²¨ì¡Œì„ ìˆ˜ ìˆì–´ìš”. í‘œí˜„ì„ ë‹¤ë“¬ì–´ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")

        status_placeholder.success(success_text)
        return full_text

    except Exception as e:
        status_placeholder.error(error_status_text)
        st.error(
            "ğŸš¨ Gemini ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
            f"**ì—ëŸ¬ ìœ í˜•**: {type(e).__name__}\n"
            f"**ë©”ì‹œì§€**: {e}\n\n"
            "â€¢ API Key/ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.\n"
            "â€¢ ì¼ì‹œì ì¸ ë„¤íŠ¸ì›Œí¬/ì„œë¹„ìŠ¤ ì´ìŠˆì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. í˜ë¥´ì†Œë‚˜ ë§¤ì¹­
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_persona(ì—…ì¢…, í”„ëœì°¨ì´ì¦ˆ, ì í¬ì—°ë ¹="ë¯¸ìƒ", ê³ ê°ì—°ë ¹ëŒ€="ë¯¸ìƒ", ê³ ê°í–‰ë™="ë¯¸ìƒ"):
    for p in personas:
        if p["ì—…ì¢…"] == ì—…ì¢… and p["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"] == í”„ëœì°¨ì´ì¦ˆ:
            return p
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ENCODED_MCT ì „ìš© ëª¨ë“œ (ìƒˆë¡œ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_mct_prompts(default_path="store_scores_with_clusterlabel_v2_with_targets_updown.csv", uploaded_file=None):
    """
    ENCODED_MCT â†’ {'prompt_str', 'analysis_prompt_updown'} ë§¤í•‘ ë¡œë“œ.
    - ì—…ë¡œë“œ íŒŒì¼ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©
    - ì—†ìœ¼ë©´ ê¸°ë³¸ ê²½ë¡œë¥¼ ì‹œë„
    """
    text = ""
    src = ""
    mapping = {}
    try:
        if uploaded_file is not None:
            text = uploaded_file.getvalue().decode("utf-8-sig")
            src = "uploaded"
        else:
            with open(default_path, "r", encoding="utf-8-sig") as f:
                text = f.read()
            src = default_path
        reader = csv.DictReader(io.StringIO(text))
        for row in reader:
            k = (row.get("ENCODED_MCT") or "").strip()
            if not k:
                continue
            mapping[k] = {
                "prompt_str": (row.get("prompt_str") or "").strip(),
                "analysis_prompt_updown": (row.get("analysis_prompt_updown") or "").strip(),
            }
        return mapping, src, None
    except Exception as e:
        return {}, src, str(e)

def build_mct_consult_prompt(info: dict, encoded_mct: str, p_main: str, p_updn: str) -> str:
    """
    ì‹ í•œì¹´ë“œ ENCODED_MCT ê¸°ë°˜ ì „ë¬¸ ì»¨ì„¤íŒ… í”„ë¡¬í”„íŠ¸.
    - ê¸°ì¡´ JSON êµ¬ì¡° ê°€ì´ë“œë¥¼ ê·¸ëŒ€ë¡œ í™œìš©(ensure_data_evidence)
    """
    name = info.get("ìƒì ëª…") or "-"
    industry = info.get("ì—…ì¢…") or "-"
    franchise = info.get("í”„ëœì°¨ì´ì¦ˆì—¬ë¶€") or "-"
    store_age = info.get("ì í¬ì—°ë ¹") or "-"
    customer_age = info.get("ê³ ê°ì—°ë ¹ëŒ€") or "-"
    behavior = info.get("ê³ ê°í–‰ë™") or "-"

    base = (
        "ë‹¹ì‹ ì€ ì†Œìƒê³µì¸ ì‹ë‹¹/ë¦¬í…Œì¼ì˜ ìš´ì˜Â·ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì‹ í•œì¹´ë“œ ENCODED_MCTë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ë°ì´í„° ê·¼ê±° ì¤‘ì‹¬ì˜ ì‹¤í–‰ ì „ëµ**ë§Œ ì œì‹œí•˜ì„¸ìš”.\n"
        "ëª¨ë“  ì „ëµì€ ê°€ëŠ¥í•œ í•œ **ì •ëŸ‰ ì§€í‘œ(%, %p, ê±´ìˆ˜, ì›)**ë¡œ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.\n\n"
        "=== ìƒì  ì¹´ë“œ ===\n"
        f"- ìƒì ëª…: {name}\n"
        f"- ì—…ì¢…: {industry}\n"
        f"- í˜•íƒœ: {franchise}\n"
        f"- ì í¬ì—°ë ¹: {store_age}\n"
        f"- ê³ ê°ì—°ë ¹ëŒ€: {customer_age}\n"
        f"- ê³ ê°í–‰ë™: {behavior}\n\n"
        "=== ENCODED_MCT ===\n"
        f"- ì½”ë“œ: {encoded_mct}\n"
        f"- ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ ìš”ì•½:\n{p_main or '-'}\n\n"
        f"- ëª©í‘œ ì—…/ë‹¤ìš´ ì§€ì‹œë¬¸:\n{p_updn or '-'}\n\n"
        "ì •ë ¬ ê·œì¹™(í•„ìˆ˜): ì œì•ˆí•œ ê° ì „ëµì€ ìœ„ 'ëª©í‘œ ì—…/ë‹¤ìš´ ì§€ì‹œë¬¸' ì¤‘ ì–´ë–¤ ì§€í‘œ(â†‘/â†“/ìœ ì§€)ë¥¼ ê²¨ëƒ¥í•˜ëŠ”ì§€ ëª…í™•íˆ ë§¤í•‘í•˜ì„¸ìš”.\n"
    )
    return ensure_data_evidence(base)

def render_mct_tab():
    """ì‚¬ì´ë“œë°” ì „í™˜í˜•: ENCODED_MCT ì „ìš© ì»¨ì„¤í„´íŠ¸ í™”ë©´"""
    st.header("ğŸ’³ ì‹ í•œì¹´ë“œ ENCODED_MCT ì»¨ì„¤í„´íŠ¸")
    st.markdown(
        "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ ì €ëŠ” **AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸**ì…ë‹ˆë‹¤.\n\n"
        "ìƒì ëª…ì„ ì…ë ¥í•´ì£¼ì‹œë©´ ì—…ì¢…ê³¼ í”„ëœì°¨ì´ì¦ˆ ì—¬ë¶€ë¥¼ ì¶”ì •í•˜ê³ , "
        "ENCODED_MCT(ìƒì  ì„¸ë¶€ ì½”ë“œ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì‹ í•œì¹´ë“œ ì„¸ë¶€ ì •ë³´**ì— ì •ë ¬ëœ ì „ë¬¸ ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.\n\n"
        "ì˜ˆ: `êµì´Œì¹˜í‚¨`, `íŒŒë¦¬ë°”ê²Œëœ¨`, `ì¹´í˜í–‰ë‹¹ì `, `ì™•ì‹­ë¦¬ë¼ì§€êµ­ë°¥`"
    )

    # ì „ìš© ì„¸ì…˜ ìƒíƒœ
    if "mct_history" not in st.session_state:
        st.session_state.mct_history = []
    if "mct_info" not in st.session_state:
        st.session_state.mct_info = {}
    if "mct_latest_strategy" not in st.session_state:
        st.session_state.mct_latest_strategy = {}

    with st.expander("ğŸ“„ ENCODED_MCT ì†ŒìŠ¤ CSV (ì„ íƒ ì—…ë¡œë“œ)", expanded=False):
        mct_csv_file = st.file_uploader("í”„ë¡¬í”„íŠ¸ CSV ì—…ë¡œë“œ", type=["csv"], key="mct_csv_uploader")
        st.caption("ê¸°ë³¸ íŒŒì¼ëª…: store_scores_with_clusterlabel_v2_with_targets_updown.csv (í”„ë¡œì íŠ¸ ë£¨íŠ¸)")

    col_a, col_b = st.columns([2, 1])
    with col_a:
        store_name = st.text_input("ìƒì ëª…", key="mct_store_name", placeholder="ì˜ˆ: êµì´Œì¹˜í‚¨ í–‰ë‹¹ì ")
        encoded_mct = st.text_input("ìƒì  ì„¸ë¶€ ì½”ë“œ (ENCODED_MCT)", key="mct_code", placeholder="ì˜ˆ: SEG_KR_05")
    with col_b:
        if store_name:
            guess_industry = classify_hpsn_mct(store_name)
            guess_fr = "í”„ëœì°¨ì´ì¦ˆ" if is_franchise(store_name) else "ê°œì¸ì í¬"
            st.metric("ì˜ˆìƒ ì—…ì¢…", guess_industry)
            st.metric("ì˜ˆìƒ í˜•íƒœ", guess_fr)

    # ì´ì „ ëŒ€í™” í‘œì‹œ
    for msg in st.session_state.mct_history:
        with st.chat_message(msg["role"]):
            if msg.get("type") == "strategy":
                c = st.container()
                render_strategy_payload(msg.get("data", {}), c, prefix=msg.get("id", "mct_hist"))
            else:
                st.markdown(msg["content"])

    # ìƒì„± ë²„íŠ¼
    generate = st.button("ğŸš€ ì „ë¬¸ ì†”ë£¨ì…˜ ìƒì„±", use_container_width=True, disabled=not encoded_mct)
    if generate:
        info = {
            "ìƒì ëª…": store_name or "",
            "ì—…ì¢…": classify_hpsn_mct(store_name) if store_name else "",
            "í”„ëœì°¨ì´ì¦ˆì—¬ë¶€": ("í”„ëœì°¨ì´ì¦ˆ" if (store_name and is_franchise(store_name)) else "ê°œì¸ì í¬") if store_name else "",
            "ì í¬ì—°ë ¹": st.session_state.mct_info.get("ì í¬ì—°ë ¹", ""),
            "ê³ ê°ì—°ë ¹ëŒ€": st.session_state.mct_info.get("ê³ ê°ì—°ë ¹ëŒ€", ""),
            "ê³ ê°í–‰ë™": st.session_state.mct_info.get("ê³ ê°í–‰ë™", ""),
        }
        st.session_state.mct_info.update({k: v for k, v in info.items() if v})

        # CSV ë¡œë“œ & ìŠ¤ë‹ˆí« ì¶”ì¶œ
        mapping, src, err = load_mct_prompts(uploaded_file=mct_csv_file)
        p_main, p_updn = "", ""
        if err:
            st.warning(f"CSV ë¡œë“œ ì‹¤íŒ¨: {err}")
        else:
            data = mapping.get(encoded_mct.strip())
            if data:
                p_main = data.get("prompt_str", "")
                p_updn = data.get("analysis_prompt_updown", "")
            else:
                st.info("í•´ë‹¹ ENCODED_MCTì— ëŒ€í•œ ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ê°€ ì—†ì–´ **ê¸°ë³¸ ë¡œì§**ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

        prompt = build_mct_consult_prompt(st.session_state.mct_info, encoded_mct, p_main, p_updn)
        with st.chat_message("assistant"):
            st.markdown("### ğŸ“ˆ ìƒì„±ëœ ë§ˆì¼€íŒ… ì „ëµ ê²°ê³¼")
            ph = st.empty()
            result = stream_gemini(
                prompt,
                output_placeholder=ph,
                status_text="ENCODED_MCT ì „ë¬¸ ì „ëµì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤... â³",
                progress_text="ì™¸ë¶€ í”„ë¡¬í”„íŠ¸ì™€ ìƒì  ì •ë³´ë¥¼ ì •ë ¬ ì¤‘... ğŸ“‹",
                success_text="âœ… ì „ëµ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            if result:
                payload = parse_strategy_payload(result)
                if payload:
                    ph.empty()
                    cid = str(uuid.uuid4())
                    box = st.container()
                    render_strategy_payload(payload, box, prefix=cid)
                    st.session_state.mct_latest_strategy = {"payload": payload, "raw": result}
                    st.session_state.mct_history.append({"role": "assistant", "type": "strategy", "data": payload, "id": cid, "raw": result})
                else:
                    st.markdown(result)
                    st.session_state.mct_latest_strategy = {"payload": None, "raw": result}
                    st.session_state.mct_history.append({"role": "assistant", "content": result})

    # í›„ì† ì§ˆë¬¸
    mct_q = st.chat_input("ENCODED_MCT ê¸°ë°˜ ì¶”ê°€ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", key="mct_chat_input")
    if mct_q:
        st.session_state.mct_history.append({"role": "user", "content": mct_q})
        latest = st.session_state.get("mct_latest_strategy", {})
        payload = latest.get("payload")
        raw = latest.get("raw", "")

        base_follow = build_followup_prompt(mct_q, st.session_state.mct_info, payload, raw)
        if encoded_mct:
            base_follow += (
                "\n\n[ENCODED_MCT íŒíŠ¸]\n"
                f"- ì½”ë“œ: {encoded_mct}\n"
                "- ìœ„ ì „ëµì˜ ê° í•­ëª©ì„ 'ëª©í‘œ ì§€í‘œ(â†‘/â†“/ìœ ì§€)'ì™€ ê³„ì† ë§¤í•‘í•˜ì„¸ìš”.\n"
            )
        with st.chat_message("assistant"):
            ph2 = st.empty()
            ans = stream_gemini(
                base_follow,
                output_placeholder=ph2,
                status_text="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ’¡",
                progress_text="ê¸°ì¡´ ì „ëµê³¼ ENCODED_MCT ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¥¼ ì¤€ë¹„ ì¤‘... ğŸ§­",
                success_text="âœ… ë‹µë³€ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            if ans:
                parsed = parse_followup_payload(ans)
                if parsed and (parsed.get("summary_points") or parsed.get("detailed_guidance")):
                    pts = (parsed.get("summary_points") or [])[:2]
                    ev = (parsed.get("evidence_mentions") or [])[:3]
                    txt = parsed.get("detailed_guidance", "")
                    guidance = "\n\n".join([s for s in ["\n".join(pts), txt] if s.strip()])
                    render_followup_panel(guidance, ev, (parsed.get("suggested_question") or ""), ui_key=1)
                    st.session_state.mct_history.append({"role": "assistant", "content": guidance})
                else:
                    st.markdown(ans)
                    st.session_state.mct_history.append({"role": "assistant", "content": ans})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. Streamlit UI ì„¤ì • (ëª¨ë“œ ì „í™˜ ì¶”ê°€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸", layout="wide")
st.title("ğŸ’¬ AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸")

# ğŸ‘‰ ì‚¬ì´ë“œë°” ëª¨ë“œ ì„ íƒ: ê¸°ì¡´ ìƒë‹´ / ENCODED_MCT ì»¨ì„¤í„´íŠ¸
mode = st.sidebar.radio("ëª¨ë“œ", ["ê¸°ì¡´ ìƒë‹´", "ENCODED_MCT ì»¨ì„¤í„´íŠ¸"], index=0)
if mode == "ENCODED_MCT ì»¨ì„¤í„´íŠ¸":
    render_mct_tab()
    st.stop()

# â”€â”€ ì´í•˜: ê¸°ì¡´ ìƒë‹´ í™”ë©´(ì›ë³¸ ë¡œì§ ìœ ì§€) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if st.button("ğŸ”„ ìƒˆ ìƒë‹´ ì‹œì‘"):
    st.session_state.clear()
    st.rerun()

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "info" not in st.session_state:
    st.session_state.info = {}
if "initialized" not in st.session_state:
    st.session_state.initialized = False
if "latest_strategy" not in st.session_state:
    st.session_state.latest_strategy = {}
if "followup_ui" not in st.session_state:
    st.session_state.followup_ui = {}
if "followup_ui_key" not in st.session_state:
    st.session_state.followup_ui_key = 0
if "auto_followup_question" not in st.session_state:
    st.session_state.auto_followup_question = None
if "shown_followup_suggestion" not in st.session_state:
    st.session_state.shown_followup_suggestion = False
if "pending_question" not in st.session_state:
    st.session_state.pending_question = None
if "use_pending_question" not in st.session_state:
    st.session_state.use_pending_question = False
if "pending_question_button_key" not in st.session_state:
    st.session_state.pending_question_button_key = 0
if "is_generating" not in st.session_state:
    st.session_state.is_generating = False

if not st.session_state.initialized:
    with st.chat_message("assistant"):
        st.markdown(
            "ì•ˆë…•í•˜ì„¸ìš” ğŸ‘‹ ì €ëŠ” **AI ë§ˆì¼€íŒ… ì»¨ì„¤í„´íŠ¸**ì…ë‹ˆë‹¤.\n\n"
            "ìƒì ëª…ì„ ì…ë ¥í•´ì£¼ì‹œë©´ ì—…ì¢…ê³¼ í”„ëœì°¨ì´ì¦ˆ ì—¬ë¶€ë¥¼ ë¶„ì„í•˜ê³ , "
            "ëª‡ ê°€ì§€ ì§ˆë¬¸ì„ í†µí•´ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆë“œë¦´ê²Œìš”.\n\n"
            "ì˜ˆ: `êµì´Œì¹˜í‚¨`, `íŒŒë¦¬ë°”ê²Œëœ¨`, `ì¹´í˜í–‰ë‹¹ì `, `ì™•ì‹­ë¦¬ë¼ì§€êµ­ë°¥`"
        )
    st.session_state.initialized = True

for msg in st.session_state.chat_history:
    with st.chat_message(msg["role"]):
        if msg.get("type") == "strategy":
            message_container = st.container()
            render_strategy_payload(msg.get("data", {}), message_container, prefix=msg.get("id", "history"))
        else:
            st.markdown(msg["content"])

pending_question = st.session_state.get("pending_question")
missing_info = get_missing_info_fields(st.session_state.info)
if pending_question and missing_info:
    with st.container():
        st.info(
            "ì¡°ê¸ˆë§Œ ë” ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤ ì „ëµì„ ë” ì •í™•íˆ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”. "
            "ì§€ê¸ˆ ì •ë³´ë§Œìœ¼ë¡œë„ ë°”ë¡œ ë‹µë³€ì„ ë°›ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."
        )
        if st.button(
            "ì´ëŒ€ë¡œ ì§ˆë¬¸",
            key=f"use_question_{st.session_state.pending_question_button_key}",
            use_container_width=True,
            disabled=st.session_state.get("is_generating", False),
        ):
            st.session_state.use_pending_question = True
            st.session_state.auto_followup_question = pending_question
            st.session_state.pending_question_button_key += 1
            st.rerun()

auto_followup_question = st.session_state.pop("auto_followup_question", None)
chat_box_value = st.chat_input("ìƒì ëª…ì„ ì…ë ¥í•˜ê±°ë‚˜ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”...")
user_input = auto_followup_question or chat_box_value

def add_message(role, content=None, **kwargs):
    message = {"role": role}
    if kwargs.get("type") == "strategy":
        message["type"] = "strategy"
        message["data"] = kwargs.get("data", {})
        message["id"] = kwargs.get("id", str(uuid.uuid4()))
        message["raw"] = kwargs.get("raw")
    else:
        message["content"] = content if content is not None else ""
    st.session_state.chat_history.append(message)

def answer_question_with_current_info(question_text: str):
    """ìˆ˜ì§‘ëœ ì •ë³´ë§Œìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€."""
    info = st.session_state.get("info", {})
    missing_fields = get_missing_info_fields(info)
    prompt = build_direct_question_prompt(info, question_text, missing_fields)

    with st.chat_message("assistant"):
        placeholder = st.empty()
        st.session_state.is_generating = True
        try:
            answer = stream_gemini(
                prompt,
                output_placeholder=placeholder,
                status_text="ì§ˆë¬¸ì— ëŒ€í•œ ì¡°ì–¸ì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ’¡",
                progress_text="ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ì¡°ì–¸ì„ ëª¨ìœ¼ê³  ìˆìŠµë‹ˆë‹¤... ğŸ§­",
                success_text="âœ… ë‹µë³€ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.",
                error_status_text="ğŸš¨ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            )
        finally:
            st.session_state.is_generating = False
        if answer:
            guidance_text, suggested_question = parse_direct_answer(answer, info, question_text)
            guidance_text = guidance_text or answer.strip()
            ui_key = st.session_state.get("followup_ui_key", 0) + 1
            st.session_state.followup_ui_key = ui_key
            st.session_state.shown_followup_suggestion = False
            st.session_state.followup_ui = {
                "guidance": guidance_text,
                "evidence": [],
                "suggested_question": suggested_question,
                "key": ui_key,
            }

            placeholder.empty()
            with st.container():
                render_followup_panel(guidance_text, [], suggested_question, ui_key)

            log_message = "### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ\n\n" + guidance_text
            if suggested_question:
                log_message += f"\n\n**ê°€ëŠ¥í•œ ë‹¤ìŒ ì§ˆë¬¸:** {suggested_question}"
            add_message("assistant", log_message)

            st.session_state.latest_strategy = {
                "payload": None,
                "raw": guidance_text,
            }
        else:
            warning_text = "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ í•´ë³´ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”."
            placeholder.warning(warning_text)
            add_message("assistant", warning_text)
            st.session_state.latest_strategy = {"payload": None, "raw": ""}

    if get_missing_info_fields(info):
        st.session_state.pending_question = question_text
    else:
        st.session_state.pending_question = None
    st.session_state.use_pending_question = False
    st.session_state.shown_followup_suggestion = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8. ëŒ€í™” ë¡œì§ (ê¸°ì¡´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if user_input:
    use_pending = st.session_state.pop("use_pending_question", False)
    st.session_state.followup_ui = {}
    add_message("user", user_input)

    if use_pending:
        answer_question_with_current_info(user_input)
        st.stop()

    # â‘  ìƒì ëª…
    if "ìƒì ëª…" not in st.session_state.info:
        info_updates, detected_question = extract_initial_store_info(user_input)
        name = info_updates.get("ìƒì ëª…") or user_input.strip()
        st.session_state.info["ìƒì ëª…"] = name
        st.session_state.info["ì—…ì¢…"] = classify_hpsn_mct(name)
        st.session_state.info["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"] = "í”„ëœì°¨ì´ì¦ˆ" if is_franchise(name) else "ê°œì¸ì í¬"

        for field in ("ì í¬ì—°ë ¹", "ê³ ê°ì—°ë ¹ëŒ€", "ê³ ê°í–‰ë™"):
            if info_updates.get(field):
                st.session_state.info[field] = info_updates[field]

        st.session_state.pending_question = detected_question or user_input
        st.session_state.shown_followup_suggestion = False

        missing_fields = get_missing_info_fields(st.session_state.info)
        if missing_fields:
            next_field = missing_fields[0]
            if next_field == "ì í¬ì—°ë ¹":
                prompt_text = (
                    f"'{name}'ì€(ëŠ”) **{st.session_state.info['ì—…ì¢…']} ì—…ì¢…**ì´ë©° "
                    f"**{st.session_state.info['í”„ëœì°¨ì´ì¦ˆì—¬ë¶€']}**ë¡œ ì¶”ì •ë©ë‹ˆë‹¤. ğŸª\n\n"
                    "ê°œì—… ì‹œê¸°ê°€ ì–¸ì œì¸ê°€ìš”? (ì˜ˆ: 6ê°œì›” ì „, 2ë…„ ì „)"
                )
            elif next_field == "ê³ ê°ì—°ë ¹ëŒ€":
                prompt_text = "ì¢‹ì•„ìš” ğŸ‘ ì£¼ìš” ê³ ê°ì¸µì€ ì–´ë–¤ ì—°ë ¹ëŒ€ì¸ê°€ìš”? (20ëŒ€ / 30~40ëŒ€ / 50ëŒ€ ì´ìƒ)"
            else:
                prompt_text = "ë§ˆì§€ë§‰ìœ¼ë¡œ, ê³ ê° ìœ í˜•ì€ ì–´ë–¤ í¸ì¸ê°€ìš”? (ì‰¼í‘œë¡œ êµ¬ë¶„ ê°€ëŠ¥: ì¬ë°©ë¬¸, ì‹ ê·œ, ì§ì¥ì¸, ìœ ë™, ê±°ì£¼)"

            add_message("assistant", prompt_text + BUTTON_HINT)
            st.rerun()
        else:
            st.session_state.pending_question_button_key += 1
            st.session_state.pending_question = st.session_state.pending_question or user_input
            st.session_state.auto_followup_question = st.session_state.pending_question
            st.session_state.use_pending_question = True
            st.rerun()

    # â‘¡ ê°œì—… ì‹œê¸°
    elif "ì í¬ì—°ë ¹" not in st.session_state.info:
        months = re.findall(r"\d+", user_input)
        months = int(months[0]) if months else 0
        if months <= 12:
            st.session_state.info["ì í¬ì—°ë ¹"] = "ì‹ ê·œ"
        elif months <= 24:
            st.session_state.info["ì í¬ì—°ë ¹"] = "ì „í™˜ê¸°"
        else:
            st.session_state.info["ì í¬ì—°ë ¹"] = "ì˜¤ë˜ëœ"

        add_message("assistant", "ì¢‹ì•„ìš” ğŸ‘ ì£¼ìš” ê³ ê°ì¸µì€ ì–´ë–¤ ì—°ë ¹ëŒ€ì¸ê°€ìš”? (20ëŒ€ / 30~40ëŒ€ / 50ëŒ€ ì´ìƒ)" + BUTTON_HINT)
        st.rerun()

    # â‘¢ ê³ ê° ì—°ë ¹ëŒ€
    elif "ê³ ê°ì—°ë ¹ëŒ€" not in st.session_state.info:
        txt = user_input
        if "20" in txt:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "20ëŒ€ ì´í•˜ ê³ ê° ì¤‘ì‹¬"
        elif "30" in txt or "40" in txt:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "30~40ëŒ€ ê³ ê° ì¤‘ì‹¬"
        else:
            st.session_state.info["ê³ ê°ì—°ë ¹ëŒ€"] = "50ëŒ€ ì´ìƒ ê³ ê° ì¤‘ì‹¬"

        add_message("assistant", "ë§ˆì§€ë§‰ìœ¼ë¡œ, ê³ ê° ìœ í˜•ì€ ì–´ë–¤ í¸ì¸ê°€ìš”? (ì‰¼í‘œë¡œ êµ¬ë¶„ ê°€ëŠ¥: ì¬ë°©ë¬¸, ì‹ ê·œ, ì§ì¥ì¸, ìœ ë™, ê±°ì£¼)" + BUTTON_HINT)
        st.rerun()

    # â‘£ ê³ ê°í–‰ë™ (ë‹¤ì¤‘ ì…ë ¥ ìœ ì—° íŒŒì‹±)
    elif "ê³ ê°í–‰ë™" not in st.session_state.info:
        txt = user_input.lower()
        parts = re.split(r"[,/+\s]*(?:ë°|ì™€|ê·¸ë¦¬ê³ )?[,/+\s]*", txt)
        parts = [p for p in parts if p]

        behaviors = []
        for p in parts:
            if "ì¬" in p or "ë‹¨ê³¨" in p: behaviors.append("ì¬ë°©ë¬¸ ê³ ê°")
            if "ì‹ " in p or "ìƒˆ" in p: behaviors.append("ì‹ ê·œ ê³ ê°")
            if "ê±°ì£¼" in p or "ì£¼ë¯¼" in p: behaviors.append("ê±°ì£¼ ê³ ê°")
            if "ì§ì¥" in p or "ì˜¤í”¼ìŠ¤" in p or "íšŒì‚¬" in p: behaviors.append("ì§ì¥ì¸ ê³ ê°")
            if "ìœ ë™" in p or "ì§€ë‚˜" in p or "ê´€ê´‘" in p: behaviors.append("ìœ ë™ ê³ ê°")

        behaviors = list(set(behaviors)) or ["ì¼ë°˜ ê³ ê°"]
        st.session_state.info["ê³ ê°í–‰ë™"] = " + ".join(behaviors)

        # persona ê¸°ë°˜ or fallback ì „ëµ ìƒì„±
        info = st.session_state.info
        persona = find_persona(
            info["ì—…ì¢…"], info["í”„ëœì°¨ì´ì¦ˆì—¬ë¶€"],
            info["ì í¬ì—°ë ¹"], info["ê³ ê°ì—°ë ¹ëŒ€"], info["ê³ ê°í–‰ë™"]
        )

        if persona and "prompt" in persona:
            prompt = ensure_data_evidence(persona["prompt"])
        else:
            prompt = ensure_data_evidence(
                "ë‹¤ìŒ ìƒì  ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 3~5ë‹¨ê³„ Phaseë³„ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆí•˜ì„¸ìš”.\n"
                "ê° PhaseëŠ” ëª©í‘œ, í•µì‹¬ ì•¡ì…˜(ì±„ë„Â·ì»¨í…ì¸ Â·ì˜¤í¼), ì˜ˆì‚°ë²”ìœ„, ì˜ˆìƒ KPI, ë‹¤ìŒ Phaseë¡œ ë„˜ì–´ê°€ëŠ” ê¸°ì¤€ì„ í¬í•¨í•˜ì„¸ìš”.\n\n"
                f"- ì—…ì¢…: {info['ì—…ì¢…']}\n"
                f"- í˜•íƒœ: {info['í”„ëœì°¨ì´ì¦ˆì—¬ë¶€']}\n"
                f"- ì í¬ì—°ë ¹: {info['ì í¬ì—°ë ¹']}\n"
                f"- ì£¼ìš” ê³ ê°ì—°ë ¹ëŒ€: {info['ê³ ê°ì—°ë ¹ëŒ€']}\n"
                f"- ê³ ê°í–‰ë™ íŠ¹ì„±: {info['ê³ ê°í–‰ë™']}\n"
                "ì‘ë‹µì€ ë¶ˆë¦¿ê³¼ í‘œë¥¼ ì ì ˆíˆ ì„ì–´ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”."
            )

        add_message("assistant", "ì´ì œ AI ìƒë‹´ì‚¬ê°€ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì„ ìƒì„±í•©ë‹ˆë‹¤... â³")

        with st.chat_message("assistant"):
            st.markdown("### ğŸ“ˆ ìƒì„±ëœ ë§ˆì¼€íŒ… ì „ëµ ê²°ê³¼")
            content_placeholder = st.empty()
            st.session_state.is_generating = True
            try:
                result = stream_gemini(prompt, output_placeholder=content_placeholder)
            finally:
                st.session_state.is_generating = False
            if result:
                payload = parse_strategy_payload(result)
                if payload:
                    message_id = str(uuid.uuid4())
                    content_placeholder.empty()
                    strategy_container = st.container()
                    render_strategy_payload(payload, strategy_container, prefix=message_id)
                    st.session_state["latest_strategy"] = {"payload": payload, "raw": result}
                    st.session_state.shown_followup_suggestion = False
                    add_message("assistant", type="strategy", data=payload, id=message_id, raw=result)
                else:
                    summary_points = extract_executive_summary(result)
                    if summary_points:
                        summary_markdown = "#### âš¡ í•µì‹¬ ìš”ì•½\n\n" + "\n".join(f"- {point}" for point in summary_points)
                        content_placeholder.markdown(summary_markdown)
                        st.session_state["latest_strategy"] = {"payload": None, "raw": result}
                        st.session_state.shown_followup_suggestion = False
                        add_message("assistant", summary_markdown)
                    else:
                        fallback_notice = "êµ¬ì¡°í™”ëœ ì‘ë‹µì„ í‘œì‹œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ í”„ë¡¬í”„íŠ¸ë¥¼ ì¡°ì •í•´ ì£¼ì„¸ìš”."
                        content_placeholder.warning(fallback_notice)
                        st.session_state["latest_strategy"] = {"payload": None, "raw": result}
                        st.session_state.shown_followup_suggestion = False
                        add_message("assistant", fallback_notice)
    else:
        latest_strategy_msg = get_latest_strategy_message()
        stored_strategy = st.session_state.get("latest_strategy", {})
        strategy_payload = None
        raw_strategy = ""

        if latest_strategy_msg:
            strategy_payload = latest_strategy_msg.get("data") or stored_strategy.get("payload")
            raw_strategy = latest_strategy_msg.get("raw") or stored_strategy.get("raw") or ""
        elif stored_strategy:
            strategy_payload = stored_strategy.get("payload")
            raw_strategy = stored_strategy.get("raw") or ""

        if not (strategy_payload or raw_strategy):
            pending_q = st.session_state.get("pending_question")
            info_state = st.session_state.get("info", {})
            if pending_q and info_state:
                answer_question_with_current_info(pending_q)
                st.rerun()

            fallback_notice = (
                "ì•„ì§ ì°¸ê³ í•  ì „ëµì´ ì—†ìŠµë‹ˆë‹¤. ìƒì  ì •ë³´ë¥¼ ì…ë ¥í•´ ë§ì¶¤ ì „ëµì„ ìƒì„±í•˜ê±°ë‚˜ "
                "ì§€ê¸ˆê¹Œì§€ì˜ ì •ë³´ë¡œ 'ì´ëŒ€ë¡œ ì§ˆë¬¸' ë²„íŠ¼ì„ ëˆŒëŸ¬ ë°”ë¡œ ì¡°ì–¸ì„ ë°›ì•„ë³´ì„¸ìš”."
            )
            add_message("assistant", fallback_notice)
            st.rerun()

        followup_prompt = build_followup_prompt(
            user_input,
            st.session_state.get("info", {}),
            strategy_payload,
            raw_strategy,
        )

        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            st.session_state.is_generating = True
            try:
                followup_answer = stream_gemini(
                    followup_prompt,
                    output_placeholder=response_placeholder,
                    status_text="ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì •ë¦¬í•˜ê³  ìˆì–´ìš”... ğŸ’¡",
                    progress_text="ê¸°ì¡´ ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì´ë“œë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤... ğŸ§­",
                    success_text="âœ… ë‹µë³€ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    error_status_text="ğŸš¨ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                )
            finally:
                st.session_state.is_generating = False
            if followup_answer:
                parsed_followup = parse_followup_payload(followup_answer)
                if parsed_followup and (parsed_followup.get("summary_points") or parsed_followup.get("detailed_guidance")):
                    summary_points = (parsed_followup.get("summary_points") or [])[:2]
                    evidence_mentions = (parsed_followup.get("evidence_mentions") or [])[:3]
                    detail_text = parsed_followup.get("detailed_guidance", "")
                    guidance_parts = []
                    if summary_points: guidance_parts.append("\n".join(point for point in summary_points))
                    if detail_text: guidance_parts.append(detail_text)
                    guidance_text = "\n\n".join(part.strip() for part in guidance_parts if part.strip())
                    guidance_text = guidance_text or detail_text or followup_answer
                    suggested_question = (parsed_followup.get("suggested_question") or "").strip()
                    if not suggested_question:
                        suggested_question = default_suggested_question(st.session_state.get("info", {}), user_input)

                    ui_key = st.session_state.get("followup_ui_key", 0) + 1
                    st.session_state.followup_ui_key = ui_key
                    st.session_state.followup_ui = {
                        "guidance": guidance_text,
                        "evidence": evidence_mentions,
                        "suggested_question": suggested_question,
                        "key": ui_key,
                    }

                    log_sections = ["### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ", guidance_text]
                    if evidence_mentions:
                        log_sections.append("**ê·¼ê±°:**\n" + "\n".join(f"- {item}" for item in evidence_mentions))
                    log_message = "\n\n".join(section for section in log_sections if section.strip())
                    if log_message:
                        add_message("assistant", log_message)

                    response_placeholder.empty()
                    with st.container():
                        render_followup_panel(guidance_text, evidence_mentions, suggested_question, ui_key)
                else:
                    ui_key = st.session_state.get("followup_ui_key", 0) + 1
                    st.session_state.followup_ui_key = ui_key
                    fallback_suggestion = default_suggested_question(st.session_state.get("info", {}), user_input)
                    st.session_state.followup_ui = {
                        "guidance": followup_answer,
                        "evidence": [],
                        "suggested_question": fallback_suggestion,
                        "key": ui_key,
                    }

                    clean_answer = (followup_answer or "").strip()
                    if clean_answer:
                        log_message = "### ğŸ“˜ ìƒì„¸ ê°€ì´ë“œ\n\n" + clean_answer
                        if fallback_suggestion:
                            log_message += f"\n\n**ê°€ëŠ¥í•œ ë‹¤ìŒ ì§ˆë¬¸:** {fallback_suggestion}"
                        add_message("assistant", log_message)

                    response_placeholder.empty()
                    with st.container():
                        render_followup_panel(followup_answer, [], fallback_suggestion, ui_key)
            else:
                warning_text = "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ í•´ë³´ì‹œë©´ ë„ì›€ì´ ë  ìˆ˜ ìˆì–´ìš”."
                response_placeholder.warning(warning_text)
else:
    active_followup_ui = st.session_state.get("followup_ui", {})
    if active_followup_ui.get("guidance"):
        guidance_text = active_followup_ui.get("guidance", "")
        evidence_items = active_followup_ui.get("evidence", [])
        suggested_question = active_followup_ui.get("suggested_question", "")
        ui_key = active_followup_ui.get("key", st.session_state.get("followup_ui_key", 0))
        with st.chat_message("assistant"):
            render_followup_panel(guidance_text, evidence_items, suggested_question, ui_key)
